[2021-12-11 15:13:06,817 DEBUG] Assigning machine gpu1 to PID 3089
[2021-12-11 15:13:06,818 DEBUG] Starting producer with PID 3088
[2021-12-11 15:13:06,819 DEBUG] Assigning machine gpu3 to PID 3092
[2021-12-11 15:13:06,820 DEBUG] gpu1 now training {'batch-size': 64, 'lr': 0.5, 'gamma': 0.5}...
[2021-12-11 15:13:06,820 DEBUG] Attempting to establish SSH connection with gpu1...
[2021-12-11 15:13:06,820 DEBUG] gpu3 now training {'batch-size': 64, 'lr': 0.5, 'gamma': 0.7}...
[2021-12-11 15:13:06,820 DEBUG] Attempting to establish SSH connection with gpu3...
[2021-12-11 15:13:06,821 DEBUG] Assigning machine gpu2 to PID 3090
[2021-12-11 15:13:06,822 DEBUG] gpu2 now training {'batch-size': 64, 'lr': 0.5, 'gamma': 0.9}...
[2021-12-11 15:13:06,822 DEBUG] Attempting to establish SSH connection with gpu2...
[2021-12-11 15:13:06,929 DEBUG] SSH connection with gpu1 established successfully!
[2021-12-11 15:13:06,929 DEBUG] Running cmd on gpu1: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "64" --lr "0.5" --gamma "0.5"
[2021-12-11 15:13:06,929 INFO] Training {'batch-size': 64, 'lr': 0.5, 'gamma': 0.5} on gpu1...
[2021-12-11 15:13:06,933 DEBUG] SSH connection with gpu2 established successfully!
[2021-12-11 15:13:06,933 DEBUG] Running cmd on gpu2: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "64" --lr "0.5" --gamma "0.9"
[2021-12-11 15:13:06,933 INFO] Training {'batch-size': 64, 'lr': 0.5, 'gamma': 0.9} on gpu2...
[2021-12-11 15:13:06,938 DEBUG] SSH connection with gpu3 established successfully!
[2021-12-11 15:13:06,939 DEBUG] Running cmd on gpu3: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "64" --lr "0.5" --gamma "0.7"
[2021-12-11 15:13:06,939 INFO] Training {'batch-size': 64, 'lr': 0.5, 'gamma': 0.7} on gpu3...
[2021-12-11 15:14:09,478 DEBUG] Received output from gpu3: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002190\n', 'Train Epoch: 1 [640/60000 (1%)]\tLoss: -298646792830976.000000\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -19602665480071385607831552.000000\n', 'Train Epoch: 1 [1920/60000 (3%)]\tLoss: -911529108758738564831772672.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -2591899768617712352113983488.000000\n', 'Train Epoch: 1 [3200/60000 (5%)]\tLoss: -4392411785232491309086801920.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -5636363397050564642342961152.000000\n', 'Train Epoch: 1 [4480/60000 (7%)]\tLoss: -7141618303761074412914212864.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -8434157754025583167068438528.000000\n', 'Train Epoch: 1 [5760/60000 (10%)]\tLoss: -10456183586139759648197050368.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -11630466125236057105615552512.000000\n', 'Train Epoch: 1 [7040/60000 (12%)]\tLoss: -12858660380692415150204911616.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -14246101098092482653279223808.000000\n', 'Train Epoch: 1 [8320/60000 (14%)]\tLoss: -15089146226755716738707357696.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -17703130732526379345368842240.000000\n', 'Train Epoch: 1 [9600/60000 (16%)]\tLoss: -18822140333123929040383639552.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -19151680052806400774919684096.000000\n', 'Train Epoch: 1 [10880/60000 (18%)]\tLoss: -20775869530275413636314824704.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -21949953729979430568634351616.000000\n', 'Train Epoch: 1 [12160/60000 (20%)]\tLoss: -21898633412226844699274510336.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -25100319968565496805014372352.000000\n', 'Train Epoch: 1 [13440/60000 (22%)]\tLoss: -25183410006831588212549353472.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -25883252913676662459106066432.000000\n', 'Train Epoch: 1 [14720/60000 (25%)]\tLoss: -28303210728357280670283726848.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -28503941999261379244559695872.000000\n', 'Train Epoch: 1 [16000/60000 (27%)]\tLoss: -31709871601884889726524063744.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -29429308601045617702764281856.000000\n', 'Train Epoch: 1 [17280/60000 (29%)]\tLoss: -31648169161419714942161911808.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -32290762819392960479792988160.000000\n', 'Train Epoch: 1 [18560/60000 (31%)]\tLoss: -34177452121362006874859241472.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -36850247798718707325600792576.000000\n', 'Train Epoch: 1 [19840/60000 (33%)]\tLoss: -36837561161162478023734198272.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -41434473256048208242683150336.000000\n', 'Train Epoch: 1 [21120/60000 (35%)]\tLoss: -38288427982793967731673661440.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -41489271596715427605742878720.000000\n', 'Train Epoch: 1 [22400/60000 (37%)]\tLoss: -42095807625408721967344779264.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -43082224981083499198872027136.000000\n', 'Train Epoch: 1 [23680/60000 (39%)]\tLoss: -43067958711938750000681451520.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -43642118196025490074698252288.000000\n', 'Train Epoch: 1 [24960/60000 (42%)]\tLoss: -44750481221387410154578771968.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -47562617405370903791818244096.000000\n', 'Train Epoch: 1 [26240/60000 (44%)]\tLoss: -46746337469340954138050035712.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -48980026260471262063418671104.000000\n', 'Train Epoch: 1 [27520/60000 (46%)]\tLoss: -49888680407272624847211986944.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -51843064832773607606416572416.000000\n', 'Train Epoch: 1 [28800/60000 (48%)]\tLoss: -49792750254539610874340966400.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -53088560658431579313657610240.000000\n', 'Train Epoch: 1 [30080/60000 (50%)]\tLoss: -54295970598405207332250189824.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -55164494074835143871017517056.000000\n', 'Train Epoch: 1 [31360/60000 (52%)]\tLoss: -52456235786375333819543715840.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -54206769817910282603808686080.000000\n', 'Train Epoch: 1 [32640/60000 (54%)]\tLoss: -53734816511612290261151907840.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -54923521157947271615053037568.000000\n', 'Train Epoch: 1 [33920/60000 (57%)]\tLoss: -57710416293079826998625304576.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -59641783904343303107058335744.000000\n', 'Train Epoch: 1 [35200/60000 (59%)]\tLoss: -62130173531727185347027664896.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -59888503941240827721247883264.000000\n', 'Train Epoch: 1 [36480/60000 (61%)]\tLoss: -60782381803317291385167544320.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -60740494412614237632122060800.000000\n', 'Train Epoch: 1 [37760/60000 (63%)]\tLoss: -63882761792682182426960592896.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -64608986199883807946503618560.000000\n', 'Train Epoch: 1 [39040/60000 (65%)]\tLoss: -65226308113623976577773600768.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -64115994750904631334419824640.000000\n', 'Train Epoch: 1 [40320/60000 (67%)]\tLoss: -60588953672178950717214556160.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -65515874181620577482987012096.000000\n', 'Train Epoch: 1 [41600/60000 (69%)]\tLoss: -69438134833664101577771712512.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -70154673667209361454981447680.000000\n', 'Train Epoch: 1 [42880/60000 (71%)]\tLoss: -68849557964705160476917497856.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -65772303404006882087735918592.000000\n', 'Train Epoch: 1 [44160/60000 (74%)]\tLoss: -69759288811064617539819536384.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -72791326712689419076079976448.000000\n', 'Train Epoch: 1 [45440/60000 (76%)]\tLoss: -73607790821012200646011518976.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -69146023410133233933788119040.000000\n', 'Train Epoch: 1 [46720/60000 (78%)]\tLoss: -70528439529595528134355058688.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -71937527574953070091088953344.000000\n', 'Train Epoch: 1 [48000/60000 (80%)]\tLoss: -77012498995999143103955992576.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -77825473275491084006074613760.000000\n', 'Train Epoch: 1 [49280/60000 (82%)]\tLoss: -75465536738807738985563029504.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -75800810591992035186799804416.000000\n', 'Train Epoch: 1 [50560/60000 (84%)]\tLoss: -78807036038421471242322182144.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -75649605139577032016702472192.000000\n', 'Train Epoch: 1 [51840/60000 (86%)]\tLoss: -76894624006220233889776926720.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -78504337069236009853769482240.000000\n', 'Train Epoch: 1 [53120/60000 (88%)]\tLoss: -79917731912825928926938267648.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -82659169548194381105683496960.000000\n', 'Train Epoch: 1 [54400/60000 (91%)]\tLoss: -81145660535167625553984356352.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -84039262263347103440805298176.000000\n', 'Train Epoch: 1 [55680/60000 (93%)]\tLoss: -75493257030062183802967425024.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -81756989765841032605478158336.000000\n', 'Train Epoch: 1 [56960/60000 (95%)]\tLoss: -77458115664422171435255988224.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -83985219501317143220979761152.000000\n', 'Train Epoch: 1 [58240/60000 (97%)]\tLoss: -87044944637631009487128100864.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -82351592374432115373625049088.000000\n', 'Train Epoch: 1 [59520/60000 (99%)]\tLoss: -82461897410738984546526560256.000000\n', '\n', 'Test set: Average loss: -85872028094041441412512219136.0000, Accuracy: 980/10000 (10%)\n', '\n', '{"accuracy": 9.8, "runtime": 60.58595617301762, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}\n']
[2021-12-11 15:14:09,478 DEBUG] Received error from gpu3: []
[2021-12-11 15:14:09,478 DEBUG] SSH connection with gpu3 has been closed
[2021-12-11 15:14:09,478 INFO] {'batch-size': 64, 'lr': 0.5, 'gamma': 0.7} => {"accuracy": 9.8, "runtime": 60.58595617301762, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}
[2021-12-11 15:14:09,480 DEBUG] gpu3 finished training {'batch-size': 64, 'lr': 0.5, 'gamma': 0.7}
[2021-12-11 15:14:09,480 DEBUG] gpu3 now training {'batch-size': 64, 'lr': 0.9, 'gamma': 0.5}...
[2021-12-11 15:14:09,480 DEBUG] Attempting to establish SSH connection with gpu3...
[2021-12-11 15:14:09,565 DEBUG] SSH connection with gpu3 established successfully!
[2021-12-11 15:14:09,565 DEBUG] Running cmd on gpu3: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "64" --lr "0.9" --gamma "0.5"
[2021-12-11 15:14:09,565 INFO] Training {'batch-size': 64, 'lr': 0.9, 'gamma': 0.5} on gpu3...
[2021-12-11 15:14:10,074 DEBUG] Received output from gpu1: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002190\n', 'Train Epoch: 1 [640/60000 (1%)]\tLoss: -298646792830976.000000\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -19602665480071385607831552.000000\n', 'Train Epoch: 1 [1920/60000 (3%)]\tLoss: -911529108758738564831772672.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -2591899768617712352113983488.000000\n', 'Train Epoch: 1 [3200/60000 (5%)]\tLoss: -4392411785232491309086801920.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -5636363397050564642342961152.000000\n', 'Train Epoch: 1 [4480/60000 (7%)]\tLoss: -7141618303761074412914212864.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -8434157754025583167068438528.000000\n', 'Train Epoch: 1 [5760/60000 (10%)]\tLoss: -10456183586139759648197050368.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -11630466125236057105615552512.000000\n', 'Train Epoch: 1 [7040/60000 (12%)]\tLoss: -12858660380692415150204911616.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -14246101098092482653279223808.000000\n', 'Train Epoch: 1 [8320/60000 (14%)]\tLoss: -15089146226755716738707357696.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -17703130732526379345368842240.000000\n', 'Train Epoch: 1 [9600/60000 (16%)]\tLoss: -18822140333123929040383639552.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -19151680052806400774919684096.000000\n', 'Train Epoch: 1 [10880/60000 (18%)]\tLoss: -20775869530275413636314824704.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -21949953729979430568634351616.000000\n', 'Train Epoch: 1 [12160/60000 (20%)]\tLoss: -21898633412226844699274510336.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -25100319968565496805014372352.000000\n', 'Train Epoch: 1 [13440/60000 (22%)]\tLoss: -25183410006831588212549353472.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -25883252913676662459106066432.000000\n', 'Train Epoch: 1 [14720/60000 (25%)]\tLoss: -28303210728357280670283726848.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -28503941999261379244559695872.000000\n', 'Train Epoch: 1 [16000/60000 (27%)]\tLoss: -31709871601884889726524063744.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -29429308601045617702764281856.000000\n', 'Train Epoch: 1 [17280/60000 (29%)]\tLoss: -31648169161419714942161911808.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -32290762819392960479792988160.000000\n', 'Train Epoch: 1 [18560/60000 (31%)]\tLoss: -34177452121362006874859241472.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -36850247798718707325600792576.000000\n', 'Train Epoch: 1 [19840/60000 (33%)]\tLoss: -36837561161162478023734198272.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -41434473256048208242683150336.000000\n', 'Train Epoch: 1 [21120/60000 (35%)]\tLoss: -38288427982793967731673661440.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -41489271596715427605742878720.000000\n', 'Train Epoch: 1 [22400/60000 (37%)]\tLoss: -42095807625408721967344779264.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -43082224981083499198872027136.000000\n', 'Train Epoch: 1 [23680/60000 (39%)]\tLoss: -43067958711938750000681451520.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -43642118196025490074698252288.000000\n', 'Train Epoch: 1 [24960/60000 (42%)]\tLoss: -44750481221387410154578771968.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -47562617405370903791818244096.000000\n', 'Train Epoch: 1 [26240/60000 (44%)]\tLoss: -46746337469340954138050035712.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -48980026260471262063418671104.000000\n', 'Train Epoch: 1 [27520/60000 (46%)]\tLoss: -49888680407272624847211986944.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -51843064832773607606416572416.000000\n', 'Train Epoch: 1 [28800/60000 (48%)]\tLoss: -49792750254539610874340966400.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -53088560658431579313657610240.000000\n', 'Train Epoch: 1 [30080/60000 (50%)]\tLoss: -54295970598405207332250189824.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -55164494074835143871017517056.000000\n', 'Train Epoch: 1 [31360/60000 (52%)]\tLoss: -52456235786375333819543715840.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -54206769817910282603808686080.000000\n', 'Train Epoch: 1 [32640/60000 (54%)]\tLoss: -53734816511612290261151907840.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -54923521157947271615053037568.000000\n', 'Train Epoch: 1 [33920/60000 (57%)]\tLoss: -57710416293079826998625304576.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -59641783904343303107058335744.000000\n', 'Train Epoch: 1 [35200/60000 (59%)]\tLoss: -62130173531727185347027664896.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -59888503941240827721247883264.000000\n', 'Train Epoch: 1 [36480/60000 (61%)]\tLoss: -60782381803317291385167544320.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -60740494412614237632122060800.000000\n', 'Train Epoch: 1 [37760/60000 (63%)]\tLoss: -63882761792682182426960592896.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -64608986199883807946503618560.000000\n', 'Train Epoch: 1 [39040/60000 (65%)]\tLoss: -65226308113623976577773600768.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -64115994750904631334419824640.000000\n', 'Train Epoch: 1 [40320/60000 (67%)]\tLoss: -60588953672178950717214556160.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -65515874181620577482987012096.000000\n', 'Train Epoch: 1 [41600/60000 (69%)]\tLoss: -69438134833664101577771712512.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -70154673667209361454981447680.000000\n', 'Train Epoch: 1 [42880/60000 (71%)]\tLoss: -68849557964705160476917497856.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -65772303404006882087735918592.000000\n', 'Train Epoch: 1 [44160/60000 (74%)]\tLoss: -69759288811064617539819536384.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -72791326712689419076079976448.000000\n', 'Train Epoch: 1 [45440/60000 (76%)]\tLoss: -73607790821012200646011518976.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -69146023410133233933788119040.000000\n', 'Train Epoch: 1 [46720/60000 (78%)]\tLoss: -70528439529595528134355058688.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -71937527574953070091088953344.000000\n', 'Train Epoch: 1 [48000/60000 (80%)]\tLoss: -77012498995999143103955992576.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -77825473275491084006074613760.000000\n', 'Train Epoch: 1 [49280/60000 (82%)]\tLoss: -75465536738807738985563029504.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -75800810591992035186799804416.000000\n', 'Train Epoch: 1 [50560/60000 (84%)]\tLoss: -78807036038421471242322182144.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -75649605139577032016702472192.000000\n', 'Train Epoch: 1 [51840/60000 (86%)]\tLoss: -76894624006220233889776926720.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -78504337069236009853769482240.000000\n', 'Train Epoch: 1 [53120/60000 (88%)]\tLoss: -79917731912825928926938267648.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -82659169548194381105683496960.000000\n', 'Train Epoch: 1 [54400/60000 (91%)]\tLoss: -81145660535167625553984356352.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -84039262263347103440805298176.000000\n', 'Train Epoch: 1 [55680/60000 (93%)]\tLoss: -75493257030062183802967425024.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -81756989765841032605478158336.000000\n', 'Train Epoch: 1 [56960/60000 (95%)]\tLoss: -77458115664422171435255988224.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -83985219501317143220979761152.000000\n', 'Train Epoch: 1 [58240/60000 (97%)]\tLoss: -87044944637631009487128100864.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -82351592374432115373625049088.000000\n', 'Train Epoch: 1 [59520/60000 (99%)]\tLoss: -82461897410738984546526560256.000000\n', '\n', 'Test set: Average loss: -85872028094041441412512219136.0000, Accuracy: 980/10000 (10%)\n', '\n', '{"accuracy": 9.8, "runtime": 60.009412026032805, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}\n']
[2021-12-11 15:14:10,074 DEBUG] Received error from gpu1: []
[2021-12-11 15:14:10,074 DEBUG] SSH connection with gpu1 has been closed
[2021-12-11 15:14:10,074 INFO] {'batch-size': 64, 'lr': 0.5, 'gamma': 0.5} => {"accuracy": 9.8, "runtime": 60.009412026032805, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}
[2021-12-11 15:14:10,076 DEBUG] gpu1 finished training {'batch-size': 64, 'lr': 0.5, 'gamma': 0.5}
[2021-12-11 15:14:10,076 DEBUG] gpu1 now training {'batch-size': 64, 'lr': 0.9, 'gamma': 0.7}...
[2021-12-11 15:14:10,076 DEBUG] Attempting to establish SSH connection with gpu1...
[2021-12-11 15:14:10,165 DEBUG] SSH connection with gpu1 established successfully!
[2021-12-11 15:14:10,165 DEBUG] Running cmd on gpu1: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "64" --lr "0.9" --gamma "0.7"
[2021-12-11 15:14:10,166 INFO] Training {'batch-size': 64, 'lr': 0.9, 'gamma': 0.7} on gpu1...
[2021-12-11 15:14:10,508 DEBUG] Received output from gpu2: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002190\n', 'Train Epoch: 1 [640/60000 (1%)]\tLoss: -298646792830976.000000\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -19602665480071385607831552.000000\n', 'Train Epoch: 1 [1920/60000 (3%)]\tLoss: -911529108758738564831772672.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -2591899768617712352113983488.000000\n', 'Train Epoch: 1 [3200/60000 (5%)]\tLoss: -4392411785232491309086801920.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -5636363397050564642342961152.000000\n', 'Train Epoch: 1 [4480/60000 (7%)]\tLoss: -7141618303761074412914212864.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -8434157754025583167068438528.000000\n', 'Train Epoch: 1 [5760/60000 (10%)]\tLoss: -10456183586139759648197050368.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -11630466125236057105615552512.000000\n', 'Train Epoch: 1 [7040/60000 (12%)]\tLoss: -12858660380692415150204911616.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -14246101098092482653279223808.000000\n', 'Train Epoch: 1 [8320/60000 (14%)]\tLoss: -15089146226755716738707357696.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -17703130732526379345368842240.000000\n', 'Train Epoch: 1 [9600/60000 (16%)]\tLoss: -18822140333123929040383639552.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -19151680052806400774919684096.000000\n', 'Train Epoch: 1 [10880/60000 (18%)]\tLoss: -20775869530275413636314824704.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -21949953729979430568634351616.000000\n', 'Train Epoch: 1 [12160/60000 (20%)]\tLoss: -21898633412226844699274510336.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -25100319968565496805014372352.000000\n', 'Train Epoch: 1 [13440/60000 (22%)]\tLoss: -25183410006831588212549353472.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -25883252913676662459106066432.000000\n', 'Train Epoch: 1 [14720/60000 (25%)]\tLoss: -28303210728357280670283726848.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -28503941999261379244559695872.000000\n', 'Train Epoch: 1 [16000/60000 (27%)]\tLoss: -31709871601884889726524063744.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -29429308601045617702764281856.000000\n', 'Train Epoch: 1 [17280/60000 (29%)]\tLoss: -31648169161419714942161911808.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -32290762819392960479792988160.000000\n', 'Train Epoch: 1 [18560/60000 (31%)]\tLoss: -34177452121362006874859241472.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -36850247798718707325600792576.000000\n', 'Train Epoch: 1 [19840/60000 (33%)]\tLoss: -36837561161162478023734198272.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -41434473256048208242683150336.000000\n', 'Train Epoch: 1 [21120/60000 (35%)]\tLoss: -38288427982793967731673661440.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -41489271596715427605742878720.000000\n', 'Train Epoch: 1 [22400/60000 (37%)]\tLoss: -42095807625408721967344779264.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -43082224981083499198872027136.000000\n', 'Train Epoch: 1 [23680/60000 (39%)]\tLoss: -43067958711938750000681451520.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -43642118196025490074698252288.000000\n', 'Train Epoch: 1 [24960/60000 (42%)]\tLoss: -44750481221387410154578771968.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -47562617405370903791818244096.000000\n', 'Train Epoch: 1 [26240/60000 (44%)]\tLoss: -46746337469340954138050035712.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -48980026260471262063418671104.000000\n', 'Train Epoch: 1 [27520/60000 (46%)]\tLoss: -49888680407272624847211986944.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -51843064832773607606416572416.000000\n', 'Train Epoch: 1 [28800/60000 (48%)]\tLoss: -49792750254539610874340966400.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -53088560658431579313657610240.000000\n', 'Train Epoch: 1 [30080/60000 (50%)]\tLoss: -54295970598405207332250189824.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -55164494074835143871017517056.000000\n', 'Train Epoch: 1 [31360/60000 (52%)]\tLoss: -52456235786375333819543715840.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -54206769817910282603808686080.000000\n', 'Train Epoch: 1 [32640/60000 (54%)]\tLoss: -53734816511612290261151907840.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -54923521157947271615053037568.000000\n', 'Train Epoch: 1 [33920/60000 (57%)]\tLoss: -57710416293079826998625304576.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -59641783904343303107058335744.000000\n', 'Train Epoch: 1 [35200/60000 (59%)]\tLoss: -62130173531727185347027664896.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -59888503941240827721247883264.000000\n', 'Train Epoch: 1 [36480/60000 (61%)]\tLoss: -60782381803317291385167544320.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -60740494412614237632122060800.000000\n', 'Train Epoch: 1 [37760/60000 (63%)]\tLoss: -63882761792682182426960592896.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -64608986199883807946503618560.000000\n', 'Train Epoch: 1 [39040/60000 (65%)]\tLoss: -65226308113623976577773600768.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -64115994750904631334419824640.000000\n', 'Train Epoch: 1 [40320/60000 (67%)]\tLoss: -60588953672178950717214556160.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -65515874181620577482987012096.000000\n', 'Train Epoch: 1 [41600/60000 (69%)]\tLoss: -69438134833664101577771712512.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -70154673667209361454981447680.000000\n', 'Train Epoch: 1 [42880/60000 (71%)]\tLoss: -68849557964705160476917497856.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -65772303404006882087735918592.000000\n', 'Train Epoch: 1 [44160/60000 (74%)]\tLoss: -69759288811064617539819536384.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -72791326712689419076079976448.000000\n', 'Train Epoch: 1 [45440/60000 (76%)]\tLoss: -73607790821012200646011518976.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -69146023410133233933788119040.000000\n', 'Train Epoch: 1 [46720/60000 (78%)]\tLoss: -70528439529595528134355058688.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -71937527574953070091088953344.000000\n', 'Train Epoch: 1 [48000/60000 (80%)]\tLoss: -77012498995999143103955992576.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -77825473275491084006074613760.000000\n', 'Train Epoch: 1 [49280/60000 (82%)]\tLoss: -75465536738807738985563029504.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -75800810591992035186799804416.000000\n', 'Train Epoch: 1 [50560/60000 (84%)]\tLoss: -78807036038421471242322182144.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -75649605139577032016702472192.000000\n', 'Train Epoch: 1 [51840/60000 (86%)]\tLoss: -76894624006220233889776926720.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -78504337069236009853769482240.000000\n', 'Train Epoch: 1 [53120/60000 (88%)]\tLoss: -79917731912825928926938267648.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -82659169548194381105683496960.000000\n', 'Train Epoch: 1 [54400/60000 (91%)]\tLoss: -81145660535167625553984356352.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -84039262263347103440805298176.000000\n', 'Train Epoch: 1 [55680/60000 (93%)]\tLoss: -75493257030062183802967425024.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -81756989765841032605478158336.000000\n', 'Train Epoch: 1 [56960/60000 (95%)]\tLoss: -77458115664422171435255988224.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -83985219501317143220979761152.000000\n', 'Train Epoch: 1 [58240/60000 (97%)]\tLoss: -87044944637631009487128100864.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -82351592374432115373625049088.000000\n', 'Train Epoch: 1 [59520/60000 (99%)]\tLoss: -82461897410738984546526560256.000000\n', '\n', 'Test set: Average loss: -85872028094041441412512219136.0000, Accuracy: 980/10000 (10%)\n', '\n', '{"accuracy": 9.8, "runtime": 60.529757336713374, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}\n']
[2021-12-11 15:14:10,508 DEBUG] Received error from gpu2: []
[2021-12-11 15:14:10,508 DEBUG] SSH connection with gpu2 has been closed
[2021-12-11 15:14:10,508 INFO] {'batch-size': 64, 'lr': 0.5, 'gamma': 0.9} => {"accuracy": 9.8, "runtime": 60.529757336713374, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}
[2021-12-11 15:14:10,510 DEBUG] gpu2 finished training {'batch-size': 64, 'lr': 0.5, 'gamma': 0.9}
[2021-12-11 15:14:10,510 DEBUG] gpu2 now training {'batch-size': 64, 'lr': 0.9, 'gamma': 0.9}...
[2021-12-11 15:14:10,510 DEBUG] Attempting to establish SSH connection with gpu2...
[2021-12-11 15:14:10,622 DEBUG] SSH connection with gpu2 established successfully!
[2021-12-11 15:14:10,622 DEBUG] Running cmd on gpu2: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "64" --lr "0.9" --gamma "0.9"
[2021-12-11 15:14:10,622 INFO] Training {'batch-size': 64, 'lr': 0.9, 'gamma': 0.9} on gpu2...
[2021-12-11 15:15:13,153 DEBUG] Received output from gpu3: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002190\n', 'Train Epoch: 1 [640/60000 (1%)]\tLoss: -199543854154645504.000000\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -65555453405024664254676992.000000\n', 'Train Epoch: 1 [1920/60000 (3%)]\tLoss: -1487441254893409930766712832.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -3369171186611727660300107776.000000\n', 'Train Epoch: 1 [3200/60000 (5%)]\tLoss: -5470845042121793936308568064.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -7128271125193053719423352832.000000\n', 'Train Epoch: 1 [4480/60000 (7%)]\tLoss: -9238274054587281332177469440.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -10770101717496898596366188544.000000\n', 'Train Epoch: 1 [5760/60000 (10%)]\tLoss: -13065747956882456266938515456.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -14784019961198060352514818048.000000\n', 'Train Epoch: 1 [7040/60000 (12%)]\tLoss: -16582823661737661215331581952.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -17890327701090573516462358528.000000\n', 'Train Epoch: 1 [8320/60000 (14%)]\tLoss: -18843399246438187465724395520.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -22160129733849268433337712640.000000\n', 'Train Epoch: 1 [9600/60000 (16%)]\tLoss: -23067102718182729623434952704.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -24394049920937236971321819136.000000\n', 'Train Epoch: 1 [10880/60000 (18%)]\tLoss: -26689795330018934904443633664.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -27477646619822009294025392128.000000\n', 'Train Epoch: 1 [12160/60000 (20%)]\tLoss: -28116566276671582247680212992.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -32042883441523891367703478272.000000\n', 'Train Epoch: 1 [13440/60000 (22%)]\tLoss: -32584409012030760758471032832.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -33695480314570606579881607168.000000\n', 'Train Epoch: 1 [14720/60000 (25%)]\tLoss: -36482757961388274404716183552.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -36315062005215090433532624896.000000\n', 'Train Epoch: 1 [16000/60000 (27%)]\tLoss: -39576085096410999631382577152.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -37511544389147358093100711936.000000\n', 'Train Epoch: 1 [17280/60000 (29%)]\tLoss: -42216497145611421490071207936.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -40334520765392279000928223232.000000\n', 'Train Epoch: 1 [18560/60000 (31%)]\tLoss: -43789035739674047606941548544.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -47390964106085075057945608192.000000\n', 'Train Epoch: 1 [19840/60000 (33%)]\tLoss: -47750520367724286974871207936.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -50834405130964514345932816384.000000\n', 'Train Epoch: 1 [21120/60000 (35%)]\tLoss: -48787023142316379663534915584.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -52849523911801683612230746112.000000\n', 'Train Epoch: 1 [22400/60000 (37%)]\tLoss: -53744086517018163374706393088.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -54393841643762680729304039424.000000\n', 'Train Epoch: 1 [23680/60000 (39%)]\tLoss: -55226890703173300493226082304.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -55014119758921125758477795328.000000\n', 'Train Epoch: 1 [24960/60000 (42%)]\tLoss: -57884019929723080895971196928.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -59154350680714464067391062016.000000\n', 'Train Epoch: 1 [26240/60000 (44%)]\tLoss: -60406675048306665281611104256.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -62619410699352480591166570496.000000\n', 'Train Epoch: 1 [27520/60000 (46%)]\tLoss: -63458773562750697070739324928.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -66771055580934914832561340416.000000\n', 'Train Epoch: 1 [28800/60000 (48%)]\tLoss: -64399572858200953529502269440.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -66534507521441491434162094080.000000\n', 'Train Epoch: 1 [30080/60000 (50%)]\tLoss: -69931249064171832800631062528.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -70754565326261258225767677952.000000\n', 'Train Epoch: 1 [31360/60000 (52%)]\tLoss: -67936441181577485878397239296.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -69639128195907999417357041664.000000\n', 'Train Epoch: 1 [32640/60000 (54%)]\tLoss: -70891225889909022888606826496.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -72940374043621750818314649600.000000\n', 'Train Epoch: 1 [33920/60000 (57%)]\tLoss: -74339568731197680868325851136.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -78358845680265278954380722176.000000\n', 'Train Epoch: 1 [35200/60000 (59%)]\tLoss: -77750217643220073339949154304.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -76836949744364946912782057472.000000\n', 'Train Epoch: 1 [36480/60000 (61%)]\tLoss: -77289513013884276492191399936.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -80172267466252602803956482048.000000\n', 'Train Epoch: 1 [37760/60000 (63%)]\tLoss: -79306941031931569014998827008.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -81024295716557875671992369152.000000\n', 'Train Epoch: 1 [39040/60000 (65%)]\tLoss: -83543139885389784253944823808.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -83258655083728751086981349376.000000\n', 'Train Epoch: 1 [40320/60000 (67%)]\tLoss: -78853886636298020992487260160.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -84046100250014298687074729984.000000\n', 'Train Epoch: 1 [41600/60000 (69%)]\tLoss: -86854935499826266442309828608.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -86668553139480367285015674880.000000\n', 'Train Epoch: 1 [42880/60000 (71%)]\tLoss: -87152274583053670783544983552.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -86116574610763665934567604224.000000\n', 'Train Epoch: 1 [44160/60000 (74%)]\tLoss: -89582631048729567953483202560.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -95707993724193853281587429376.000000\n', 'Train Epoch: 1 [45440/60000 (76%)]\tLoss: -94174726329802771134314184704.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -91526054861623810266146799616.000000\n', 'Train Epoch: 1 [46720/60000 (78%)]\tLoss: -93366767203515637795412508672.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -94132659489173368334750580736.000000\n', 'Train Epoch: 1 [48000/60000 (80%)]\tLoss: -97037273775990898234209861632.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -100990631211324118942726750208.000000\n', 'Train Epoch: 1 [49280/60000 (82%)]\tLoss: -97997642558146166502738362368.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -96023495028914374278314459136.000000\n', 'Train Epoch: 1 [50560/60000 (84%)]\tLoss: -101383635994761496556700958720.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -98623101109336319532711542784.000000\n', 'Train Epoch: 1 [51840/60000 (86%)]\tLoss: -99992237934248784290937569280.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -101798779232270567067436974080.000000\n', 'Train Epoch: 1 [53120/60000 (88%)]\tLoss: -102534061138386336566499868672.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -106136943422826903685837094912.000000\n', 'Train Epoch: 1 [54400/60000 (91%)]\tLoss: -104989767267342277991945207808.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -108048264588370598150337986560.000000\n', 'Train Epoch: 1 [55680/60000 (93%)]\tLoss: -100655994877615010143593824256.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -106036800919191169989435457536.000000\n', 'Train Epoch: 1 [56960/60000 (95%)]\tLoss: -103575919076570003432835907584.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -107598289175683881136505749504.000000\n', 'Train Epoch: 1 [58240/60000 (97%)]\tLoss: -111593014541407039937094615040.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -108772250593859343458049720320.000000\n', 'Train Epoch: 1 [59520/60000 (99%)]\tLoss: -105442132197469327046255575040.000000\n', '\n', 'Test set: Average loss: -111653196168302719806667227136.0000, Accuracy: 980/10000 (10%)\n', '\n', '{"accuracy": 9.8, "runtime": 61.567034523934126, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}\n']
[2021-12-11 15:15:13,153 DEBUG] Received error from gpu3: []
[2021-12-11 15:15:13,153 DEBUG] SSH connection with gpu3 has been closed
[2021-12-11 15:15:13,153 INFO] {'batch-size': 64, 'lr': 0.9, 'gamma': 0.5} => {"accuracy": 9.8, "runtime": 61.567034523934126, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}
[2021-12-11 15:15:13,154 DEBUG] gpu3 finished training {'batch-size': 64, 'lr': 0.9, 'gamma': 0.5}
[2021-12-11 15:15:13,154 DEBUG] gpu3 now training {'batch-size': 64, 'lr': 0.99, 'gamma': 0.5}...
[2021-12-11 15:15:13,154 DEBUG] Attempting to establish SSH connection with gpu3...
[2021-12-11 15:15:13,247 DEBUG] SSH connection with gpu3 established successfully!
[2021-12-11 15:15:13,247 DEBUG] Running cmd on gpu3: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "64" --lr "0.99" --gamma "0.5"
[2021-12-11 15:15:13,247 INFO] Training {'batch-size': 64, 'lr': 0.99, 'gamma': 0.5} on gpu3...
[2021-12-11 15:15:13,416 DEBUG] Received output from gpu1: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002190\n', 'Train Epoch: 1 [640/60000 (1%)]\tLoss: -199543854154645504.000000\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -65555453405024664254676992.000000\n', 'Train Epoch: 1 [1920/60000 (3%)]\tLoss: -1487441254893409930766712832.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -3369171186611727660300107776.000000\n', 'Train Epoch: 1 [3200/60000 (5%)]\tLoss: -5470845042121793936308568064.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -7128271125193053719423352832.000000\n', 'Train Epoch: 1 [4480/60000 (7%)]\tLoss: -9238274054587281332177469440.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -10770101717496898596366188544.000000\n', 'Train Epoch: 1 [5760/60000 (10%)]\tLoss: -13065747956882456266938515456.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -14784019961198060352514818048.000000\n', 'Train Epoch: 1 [7040/60000 (12%)]\tLoss: -16582823661737661215331581952.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -17890327701090573516462358528.000000\n', 'Train Epoch: 1 [8320/60000 (14%)]\tLoss: -18843399246438187465724395520.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -22160129733849268433337712640.000000\n', 'Train Epoch: 1 [9600/60000 (16%)]\tLoss: -23067102718182729623434952704.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -24394049920937236971321819136.000000\n', 'Train Epoch: 1 [10880/60000 (18%)]\tLoss: -26689795330018934904443633664.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -27477646619822009294025392128.000000\n', 'Train Epoch: 1 [12160/60000 (20%)]\tLoss: -28116566276671582247680212992.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -32042883441523891367703478272.000000\n', 'Train Epoch: 1 [13440/60000 (22%)]\tLoss: -32584409012030760758471032832.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -33695480314570606579881607168.000000\n', 'Train Epoch: 1 [14720/60000 (25%)]\tLoss: -36482757961388274404716183552.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -36315062005215090433532624896.000000\n', 'Train Epoch: 1 [16000/60000 (27%)]\tLoss: -39576085096410999631382577152.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -37511544389147358093100711936.000000\n', 'Train Epoch: 1 [17280/60000 (29%)]\tLoss: -42216497145611421490071207936.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -40334520765392279000928223232.000000\n', 'Train Epoch: 1 [18560/60000 (31%)]\tLoss: -43789035739674047606941548544.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -47390964106085075057945608192.000000\n', 'Train Epoch: 1 [19840/60000 (33%)]\tLoss: -47750520367724286974871207936.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -50834405130964514345932816384.000000\n', 'Train Epoch: 1 [21120/60000 (35%)]\tLoss: -48787023142316379663534915584.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -52849523911801683612230746112.000000\n', 'Train Epoch: 1 [22400/60000 (37%)]\tLoss: -53744086517018163374706393088.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -54393841643762680729304039424.000000\n', 'Train Epoch: 1 [23680/60000 (39%)]\tLoss: -55226890703173300493226082304.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -55014119758921125758477795328.000000\n', 'Train Epoch: 1 [24960/60000 (42%)]\tLoss: -57884019929723080895971196928.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -59154350680714464067391062016.000000\n', 'Train Epoch: 1 [26240/60000 (44%)]\tLoss: -60406675048306665281611104256.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -62619410699352480591166570496.000000\n', 'Train Epoch: 1 [27520/60000 (46%)]\tLoss: -63458773562750697070739324928.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -66771055580934914832561340416.000000\n', 'Train Epoch: 1 [28800/60000 (48%)]\tLoss: -64399572858200953529502269440.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -66534507521441491434162094080.000000\n', 'Train Epoch: 1 [30080/60000 (50%)]\tLoss: -69931249064171832800631062528.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -70754565326261258225767677952.000000\n', 'Train Epoch: 1 [31360/60000 (52%)]\tLoss: -67936441181577485878397239296.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -69639128195907999417357041664.000000\n', 'Train Epoch: 1 [32640/60000 (54%)]\tLoss: -70891225889909022888606826496.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -72940374043621750818314649600.000000\n', 'Train Epoch: 1 [33920/60000 (57%)]\tLoss: -74339568731197680868325851136.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -78358845680265278954380722176.000000\n', 'Train Epoch: 1 [35200/60000 (59%)]\tLoss: -77750217643220073339949154304.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -76836949744364946912782057472.000000\n', 'Train Epoch: 1 [36480/60000 (61%)]\tLoss: -77289513013884276492191399936.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -80172267466252602803956482048.000000\n', 'Train Epoch: 1 [37760/60000 (63%)]\tLoss: -79306941031931569014998827008.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -81024295716557875671992369152.000000\n', 'Train Epoch: 1 [39040/60000 (65%)]\tLoss: -83543139885389784253944823808.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -83258655083728751086981349376.000000\n', 'Train Epoch: 1 [40320/60000 (67%)]\tLoss: -78853886636298020992487260160.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -84046100250014298687074729984.000000\n', 'Train Epoch: 1 [41600/60000 (69%)]\tLoss: -86854935499826266442309828608.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -86668553139480367285015674880.000000\n', 'Train Epoch: 1 [42880/60000 (71%)]\tLoss: -87152274583053670783544983552.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -86116574610763665934567604224.000000\n', 'Train Epoch: 1 [44160/60000 (74%)]\tLoss: -89582631048729567953483202560.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -95707993724193853281587429376.000000\n', 'Train Epoch: 1 [45440/60000 (76%)]\tLoss: -94174726329802771134314184704.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -91526054861623810266146799616.000000\n', 'Train Epoch: 1 [46720/60000 (78%)]\tLoss: -93366767203515637795412508672.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -94132659489173368334750580736.000000\n', 'Train Epoch: 1 [48000/60000 (80%)]\tLoss: -97037273775990898234209861632.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -100990631211324118942726750208.000000\n', 'Train Epoch: 1 [49280/60000 (82%)]\tLoss: -97997642558146166502738362368.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -96023495028914374278314459136.000000\n', 'Train Epoch: 1 [50560/60000 (84%)]\tLoss: -101383635994761496556700958720.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -98623101109336319532711542784.000000\n', 'Train Epoch: 1 [51840/60000 (86%)]\tLoss: -99992237934248784290937569280.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -101798779232270567067436974080.000000\n', 'Train Epoch: 1 [53120/60000 (88%)]\tLoss: -102534061138386336566499868672.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -106136943422826903685837094912.000000\n', 'Train Epoch: 1 [54400/60000 (91%)]\tLoss: -104989767267342277991945207808.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -108048264588370598150337986560.000000\n', 'Train Epoch: 1 [55680/60000 (93%)]\tLoss: -100655994877615010143593824256.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -106036800919191169989435457536.000000\n', 'Train Epoch: 1 [56960/60000 (95%)]\tLoss: -103575919076570003432835907584.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -107598289175683881136505749504.000000\n', 'Train Epoch: 1 [58240/60000 (97%)]\tLoss: -111593014541407039937094615040.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -108772250593859343458049720320.000000\n', 'Train Epoch: 1 [59520/60000 (99%)]\tLoss: -105442132197469327046255575040.000000\n', '\n', 'Test set: Average loss: -111653196168302719806667227136.0000, Accuracy: 980/10000 (10%)\n', '\n', '{"accuracy": 9.8, "runtime": 60.26496073510498, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}\n']
[2021-12-11 15:15:13,416 DEBUG] Received error from gpu1: []
[2021-12-11 15:15:13,416 DEBUG] SSH connection with gpu1 has been closed
[2021-12-11 15:15:13,416 INFO] {'batch-size': 64, 'lr': 0.9, 'gamma': 0.7} => {"accuracy": 9.8, "runtime": 60.26496073510498, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}
[2021-12-11 15:15:13,416 DEBUG] gpu1 finished training {'batch-size': 64, 'lr': 0.9, 'gamma': 0.7}
[2021-12-11 15:15:13,417 DEBUG] gpu1 now training {'batch-size': 64, 'lr': 0.99, 'gamma': 0.7}...
[2021-12-11 15:15:13,417 DEBUG] Attempting to establish SSH connection with gpu1...
[2021-12-11 15:15:13,501 DEBUG] SSH connection with gpu1 established successfully!
[2021-12-11 15:15:13,501 DEBUG] Running cmd on gpu1: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "64" --lr "0.99" --gamma "0.7"
[2021-12-11 15:15:13,501 INFO] Training {'batch-size': 64, 'lr': 0.99, 'gamma': 0.7} on gpu1...
[2021-12-11 15:15:13,677 DEBUG] Received output from gpu2: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002190\n', 'Train Epoch: 1 [640/60000 (1%)]\tLoss: -199543854154645504.000000\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -65555453405024664254676992.000000\n', 'Train Epoch: 1 [1920/60000 (3%)]\tLoss: -1487441254893409930766712832.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -3369171186611727660300107776.000000\n', 'Train Epoch: 1 [3200/60000 (5%)]\tLoss: -5470845042121793936308568064.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -7128271125193053719423352832.000000\n', 'Train Epoch: 1 [4480/60000 (7%)]\tLoss: -9238274054587281332177469440.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -10770101717496898596366188544.000000\n', 'Train Epoch: 1 [5760/60000 (10%)]\tLoss: -13065747956882456266938515456.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -14784019961198060352514818048.000000\n', 'Train Epoch: 1 [7040/60000 (12%)]\tLoss: -16582823661737661215331581952.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -17890327701090573516462358528.000000\n', 'Train Epoch: 1 [8320/60000 (14%)]\tLoss: -18843399246438187465724395520.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -22160129733849268433337712640.000000\n', 'Train Epoch: 1 [9600/60000 (16%)]\tLoss: -23067102718182729623434952704.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -24394049920937236971321819136.000000\n', 'Train Epoch: 1 [10880/60000 (18%)]\tLoss: -26689795330018934904443633664.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -27477646619822009294025392128.000000\n', 'Train Epoch: 1 [12160/60000 (20%)]\tLoss: -28116566276671582247680212992.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -32042883441523891367703478272.000000\n', 'Train Epoch: 1 [13440/60000 (22%)]\tLoss: -32584409012030760758471032832.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -33695480314570606579881607168.000000\n', 'Train Epoch: 1 [14720/60000 (25%)]\tLoss: -36482757961388274404716183552.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -36315062005215090433532624896.000000\n', 'Train Epoch: 1 [16000/60000 (27%)]\tLoss: -39576085096410999631382577152.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -37511544389147358093100711936.000000\n', 'Train Epoch: 1 [17280/60000 (29%)]\tLoss: -42216497145611421490071207936.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -40334520765392279000928223232.000000\n', 'Train Epoch: 1 [18560/60000 (31%)]\tLoss: -43789035739674047606941548544.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -47390964106085075057945608192.000000\n', 'Train Epoch: 1 [19840/60000 (33%)]\tLoss: -47750520367724286974871207936.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -50834405130964514345932816384.000000\n', 'Train Epoch: 1 [21120/60000 (35%)]\tLoss: -48787023142316379663534915584.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -52849523911801683612230746112.000000\n', 'Train Epoch: 1 [22400/60000 (37%)]\tLoss: -53744086517018163374706393088.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -54393841643762680729304039424.000000\n', 'Train Epoch: 1 [23680/60000 (39%)]\tLoss: -55226890703173300493226082304.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -55014119758921125758477795328.000000\n', 'Train Epoch: 1 [24960/60000 (42%)]\tLoss: -57884019929723080895971196928.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -59154350680714464067391062016.000000\n', 'Train Epoch: 1 [26240/60000 (44%)]\tLoss: -60406675048306665281611104256.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -62619410699352480591166570496.000000\n', 'Train Epoch: 1 [27520/60000 (46%)]\tLoss: -63458773562750697070739324928.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -66771055580934914832561340416.000000\n', 'Train Epoch: 1 [28800/60000 (48%)]\tLoss: -64399572858200953529502269440.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -66534507521441491434162094080.000000\n', 'Train Epoch: 1 [30080/60000 (50%)]\tLoss: -69931249064171832800631062528.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -70754565326261258225767677952.000000\n', 'Train Epoch: 1 [31360/60000 (52%)]\tLoss: -67936441181577485878397239296.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -69639128195907999417357041664.000000\n', 'Train Epoch: 1 [32640/60000 (54%)]\tLoss: -70891225889909022888606826496.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -72940374043621750818314649600.000000\n', 'Train Epoch: 1 [33920/60000 (57%)]\tLoss: -74339568731197680868325851136.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -78358845680265278954380722176.000000\n', 'Train Epoch: 1 [35200/60000 (59%)]\tLoss: -77750217643220073339949154304.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -76836949744364946912782057472.000000\n', 'Train Epoch: 1 [36480/60000 (61%)]\tLoss: -77289513013884276492191399936.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -80172267466252602803956482048.000000\n', 'Train Epoch: 1 [37760/60000 (63%)]\tLoss: -79306941031931569014998827008.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -81024295716557875671992369152.000000\n', 'Train Epoch: 1 [39040/60000 (65%)]\tLoss: -83543139885389784253944823808.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -83258655083728751086981349376.000000\n', 'Train Epoch: 1 [40320/60000 (67%)]\tLoss: -78853886636298020992487260160.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -84046100250014298687074729984.000000\n', 'Train Epoch: 1 [41600/60000 (69%)]\tLoss: -86854935499826266442309828608.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -86668553139480367285015674880.000000\n', 'Train Epoch: 1 [42880/60000 (71%)]\tLoss: -87152274583053670783544983552.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -86116574610763665934567604224.000000\n', 'Train Epoch: 1 [44160/60000 (74%)]\tLoss: -89582631048729567953483202560.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -95707993724193853281587429376.000000\n', 'Train Epoch: 1 [45440/60000 (76%)]\tLoss: -94174726329802771134314184704.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -91526054861623810266146799616.000000\n', 'Train Epoch: 1 [46720/60000 (78%)]\tLoss: -93366767203515637795412508672.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -94132659489173368334750580736.000000\n', 'Train Epoch: 1 [48000/60000 (80%)]\tLoss: -97037273775990898234209861632.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -100990631211324118942726750208.000000\n', 'Train Epoch: 1 [49280/60000 (82%)]\tLoss: -97997642558146166502738362368.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -96023495028914374278314459136.000000\n', 'Train Epoch: 1 [50560/60000 (84%)]\tLoss: -101383635994761496556700958720.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -98623101109336319532711542784.000000\n', 'Train Epoch: 1 [51840/60000 (86%)]\tLoss: -99992237934248784290937569280.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -101798779232270567067436974080.000000\n', 'Train Epoch: 1 [53120/60000 (88%)]\tLoss: -102534061138386336566499868672.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -106136943422826903685837094912.000000\n', 'Train Epoch: 1 [54400/60000 (91%)]\tLoss: -104989767267342277991945207808.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -108048264588370598150337986560.000000\n', 'Train Epoch: 1 [55680/60000 (93%)]\tLoss: -100655994877615010143593824256.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -106036800919191169989435457536.000000\n', 'Train Epoch: 1 [56960/60000 (95%)]\tLoss: -103575919076570003432835907584.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -107598289175683881136505749504.000000\n', 'Train Epoch: 1 [58240/60000 (97%)]\tLoss: -111593014541407039937094615040.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -108772250593859343458049720320.000000\n', 'Train Epoch: 1 [59520/60000 (99%)]\tLoss: -105442132197469327046255575040.000000\n', '\n', 'Test set: Average loss: -111653196168302719806667227136.0000, Accuracy: 980/10000 (10%)\n', '\n', '{"accuracy": 9.8, "runtime": 60.04498504800722, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}\n']
[2021-12-11 15:15:13,677 DEBUG] Received error from gpu2: []
[2021-12-11 15:15:13,677 DEBUG] SSH connection with gpu2 has been closed
[2021-12-11 15:15:13,677 INFO] {'batch-size': 64, 'lr': 0.9, 'gamma': 0.9} => {"accuracy": 9.8, "runtime": 60.04498504800722, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}
[2021-12-11 15:15:13,678 DEBUG] gpu2 finished training {'batch-size': 64, 'lr': 0.9, 'gamma': 0.9}
[2021-12-11 15:15:13,678 DEBUG] gpu2 now training {'batch-size': 64, 'lr': 0.99, 'gamma': 0.9}...
[2021-12-11 15:15:13,678 DEBUG] Attempting to establish SSH connection with gpu2...
[2021-12-11 15:15:13,766 DEBUG] SSH connection with gpu2 established successfully!
[2021-12-11 15:15:13,766 DEBUG] Running cmd on gpu2: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "64" --lr "0.99" --gamma "0.9"
[2021-12-11 15:15:13,766 INFO] Training {'batch-size': 64, 'lr': 0.99, 'gamma': 0.9} on gpu2...
[2021-12-11 15:16:15,652 DEBUG] Received output from gpu3: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002190\n', 'Train Epoch: 1 [640/60000 (1%)]\tLoss: -515892676818632704.000000\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -91063265233698850175188992.000000\n', 'Train Epoch: 1 [1920/60000 (3%)]\tLoss: -1487482723174087629838745600.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -3146413912987809522688983040.000000\n', 'Train Epoch: 1 [3200/60000 (5%)]\tLoss: -5008643422610927411018072064.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -6239527656075090077262282752.000000\n', 'Train Epoch: 1 [4480/60000 (7%)]\tLoss: -8094218202756211756360531968.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -9121254403437581882487734272.000000\n', 'Train Epoch: 1 [5760/60000 (10%)]\tLoss: -11323722449158017865937321984.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -12718703605239607475006603264.000000\n', 'Train Epoch: 1 [7040/60000 (12%)]\tLoss: -14619683968777438133900804096.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -15467662789823650281165946880.000000\n', 'Train Epoch: 1 [8320/60000 (14%)]\tLoss: -15895587112218607486672830464.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -18738464721413961796842160128.000000\n', 'Train Epoch: 1 [9600/60000 (16%)]\tLoss: -19821214811564417638494896128.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -21090098157228801263656763392.000000\n', 'Train Epoch: 1 [10880/60000 (18%)]\tLoss: -22768531097303296304799023104.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -23172508299263621539427254272.000000\n', 'Train Epoch: 1 [12160/60000 (20%)]\tLoss: -23747383221872516364693929984.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -26436914966044506838072819712.000000\n', 'Train Epoch: 1 [13440/60000 (22%)]\tLoss: -26916324889018950350522023936.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -28719043431372850149220941824.000000\n', 'Train Epoch: 1 [14720/60000 (25%)]\tLoss: -30206206361791675950272872448.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -31047054409448754933265334272.000000\n', 'Train Epoch: 1 [16000/60000 (27%)]\tLoss: -32035913228595175771368062976.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -31223009784600477913927647232.000000\n', 'Train Epoch: 1 [17280/60000 (29%)]\tLoss: -34667348059111662434505457664.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -33237604382758048649606856704.000000\n', 'Train Epoch: 1 [18560/60000 (31%)]\tLoss: -36686225843669195938393489408.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -38397979801646819196163588096.000000\n', 'Train Epoch: 1 [19840/60000 (33%)]\tLoss: -40130146188746646495369887744.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -44574506956769755695335604224.000000\n', 'Train Epoch: 1 [21120/60000 (35%)]\tLoss: -39802994805912886083900669952.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -43047789484690413745973755904.000000\n', 'Train Epoch: 1 [22400/60000 (37%)]\tLoss: -44911424193490090533106745344.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -45362032403285512079397617664.000000\n', 'Train Epoch: 1 [23680/60000 (39%)]\tLoss: -46470480431244123812891983872.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -46160570408439320476097970176.000000\n', 'Train Epoch: 1 [24960/60000 (42%)]\tLoss: -48130477252569495759119974400.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -50518984106474202133174419456.000000\n', 'Train Epoch: 1 [26240/60000 (44%)]\tLoss: -50461522351110644290214166528.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -51936699915395946931713736704.000000\n', 'Train Epoch: 1 [27520/60000 (46%)]\tLoss: -51554622687999929676764020736.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -57731917227676332493283262464.000000\n', 'Train Epoch: 1 [28800/60000 (48%)]\tLoss: -52650783118370114593412349952.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -57003822763347490594235613184.000000\n', 'Train Epoch: 1 [30080/60000 (50%)]\tLoss: -59303577461573144856143855616.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -59283479069822051646114365440.000000\n', 'Train Epoch: 1 [31360/60000 (52%)]\tLoss: -55061466205278376821390311424.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -59320785765036721843302563840.000000\n', 'Train Epoch: 1 [32640/60000 (54%)]\tLoss: -58551904221395334818544222208.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -58795814450235551993831620608.000000\n', 'Train Epoch: 1 [33920/60000 (57%)]\tLoss: -62447955739458932382392909824.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -64170953652032268265416818688.000000\n', 'Train Epoch: 1 [35200/60000 (59%)]\tLoss: -64199490912688249531443183616.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -65573165931790752018719571968.000000\n', 'Train Epoch: 1 [36480/60000 (61%)]\tLoss: -62385847175476230808542380032.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -66543088061340865579515379712.000000\n', 'Train Epoch: 1 [37760/60000 (63%)]\tLoss: -68046146477341030606356611072.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -68279733613049694737206870016.000000\n', 'Train Epoch: 1 [39040/60000 (65%)]\tLoss: -68392739842984765347170615296.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -66020718770471756904557182976.000000\n', 'Train Epoch: 1 [40320/60000 (67%)]\tLoss: -64910273181490891311137423360.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -70319843157314210165975678976.000000\n', 'Train Epoch: 1 [41600/60000 (69%)]\tLoss: -73260002696648299884958449664.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -73964093372144715377384882176.000000\n', 'Train Epoch: 1 [42880/60000 (71%)]\tLoss: -74609215857369537610027892736.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -70301680935821093510483804160.000000\n', 'Train Epoch: 1 [44160/60000 (74%)]\tLoss: -70990518367577840048869998592.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -78247865345551359422213652480.000000\n', 'Train Epoch: 1 [45440/60000 (76%)]\tLoss: -77075929822597048178466357248.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -74285894313753384480826982400.000000\n', 'Train Epoch: 1 [46720/60000 (78%)]\tLoss: -78705528770872188218453786624.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -74823653796990165329536614400.000000\n', 'Train Epoch: 1 [48000/60000 (80%)]\tLoss: -79730461747581250276343939072.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -83273823324871728387407740928.000000\n', 'Train Epoch: 1 [49280/60000 (82%)]\tLoss: -81781593295216783457041514496.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -78081288590234615556945739776.000000\n', 'Train Epoch: 1 [50560/60000 (84%)]\tLoss: -82117636894137787410448121856.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -81640007303327385754244481024.000000\n', 'Train Epoch: 1 [51840/60000 (86%)]\tLoss: -85267039769961348239204548608.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -83486329816600862422024060928.000000\n', 'Train Epoch: 1 [53120/60000 (88%)]\tLoss: -83382447198710695966613176320.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -88423214519321344139197415424.000000\n', 'Train Epoch: 1 [54400/60000 (91%)]\tLoss: -85895700085626886888632614912.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -90541091994825756885345370112.000000\n', 'Train Epoch: 1 [55680/60000 (93%)]\tLoss: -81870817687544122533709086720.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -85937020792351996284252454912.000000\n', 'Train Epoch: 1 [56960/60000 (95%)]\tLoss: -87649860894620679560421703680.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -88462523497924751065956220928.000000\n', 'Train Epoch: 1 [58240/60000 (97%)]\tLoss: -93880504008454060758920069120.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -90146387159454546199094231040.000000\n', 'Train Epoch: 1 [59520/60000 (99%)]\tLoss: -87638102202078334143839600640.000000\n', '\n', 'Test set: Average loss: -91465940632342573349200723968.0000, Accuracy: 980/10000 (10%)\n', '\n', '{"accuracy": 9.8, "runtime": 60.48147195391357, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}\n']
[2021-12-11 15:16:15,652 DEBUG] Received error from gpu3: []
[2021-12-11 15:16:15,653 DEBUG] SSH connection with gpu3 has been closed
[2021-12-11 15:16:15,653 INFO] {'batch-size': 64, 'lr': 0.99, 'gamma': 0.5} => {"accuracy": 9.8, "runtime": 60.48147195391357, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}
[2021-12-11 15:16:15,653 DEBUG] gpu3 finished training {'batch-size': 64, 'lr': 0.99, 'gamma': 0.5}
[2021-12-11 15:16:15,654 DEBUG] gpu3 now training {'batch-size': 128, 'lr': 0.5, 'gamma': 0.5}...
[2021-12-11 15:16:15,654 DEBUG] Attempting to establish SSH connection with gpu3...
[2021-12-11 15:16:15,757 DEBUG] SSH connection with gpu3 established successfully!
[2021-12-11 15:16:15,757 DEBUG] Running cmd on gpu3: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "128" --lr "0.5" --gamma "0.5"
[2021-12-11 15:16:15,757 INFO] Training {'batch-size': 128, 'lr': 0.5, 'gamma': 0.5} on gpu3...
[2021-12-11 15:16:16,325 DEBUG] Received output from gpu1: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002190\n', 'Train Epoch: 1 [640/60000 (1%)]\tLoss: -515892676818632704.000000\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -91063265233698850175188992.000000\n', 'Train Epoch: 1 [1920/60000 (3%)]\tLoss: -1487482723174087629838745600.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -3146413912987809522688983040.000000\n', 'Train Epoch: 1 [3200/60000 (5%)]\tLoss: -5008643422610927411018072064.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -6239527656075090077262282752.000000\n', 'Train Epoch: 1 [4480/60000 (7%)]\tLoss: -8094218202756211756360531968.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -9121254403437581882487734272.000000\n', 'Train Epoch: 1 [5760/60000 (10%)]\tLoss: -11323722449158017865937321984.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -12718703605239607475006603264.000000\n', 'Train Epoch: 1 [7040/60000 (12%)]\tLoss: -14619683968777438133900804096.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -15467662789823650281165946880.000000\n', 'Train Epoch: 1 [8320/60000 (14%)]\tLoss: -15895587112218607486672830464.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -18738464721413961796842160128.000000\n', 'Train Epoch: 1 [9600/60000 (16%)]\tLoss: -19821214811564417638494896128.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -21090098157228801263656763392.000000\n', 'Train Epoch: 1 [10880/60000 (18%)]\tLoss: -22768531097303296304799023104.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -23172508299263621539427254272.000000\n', 'Train Epoch: 1 [12160/60000 (20%)]\tLoss: -23747383221872516364693929984.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -26436914966044506838072819712.000000\n', 'Train Epoch: 1 [13440/60000 (22%)]\tLoss: -26916324889018950350522023936.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -28719043431372850149220941824.000000\n', 'Train Epoch: 1 [14720/60000 (25%)]\tLoss: -30206206361791675950272872448.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -31047054409448754933265334272.000000\n', 'Train Epoch: 1 [16000/60000 (27%)]\tLoss: -32035913228595175771368062976.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -31223009784600477913927647232.000000\n', 'Train Epoch: 1 [17280/60000 (29%)]\tLoss: -34667348059111662434505457664.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -33237604382758048649606856704.000000\n', 'Train Epoch: 1 [18560/60000 (31%)]\tLoss: -36686225843669195938393489408.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -38397979801646819196163588096.000000\n', 'Train Epoch: 1 [19840/60000 (33%)]\tLoss: -40130146188746646495369887744.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -44574506956769755695335604224.000000\n', 'Train Epoch: 1 [21120/60000 (35%)]\tLoss: -39802994805912886083900669952.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -43047789484690413745973755904.000000\n', 'Train Epoch: 1 [22400/60000 (37%)]\tLoss: -44911424193490090533106745344.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -45362032403285512079397617664.000000\n', 'Train Epoch: 1 [23680/60000 (39%)]\tLoss: -46470480431244123812891983872.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -46160570408439320476097970176.000000\n', 'Train Epoch: 1 [24960/60000 (42%)]\tLoss: -48130477252569495759119974400.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -50518984106474202133174419456.000000\n', 'Train Epoch: 1 [26240/60000 (44%)]\tLoss: -50461522351110644290214166528.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -51936699915395946931713736704.000000\n', 'Train Epoch: 1 [27520/60000 (46%)]\tLoss: -51554622687999929676764020736.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -57731917227676332493283262464.000000\n', 'Train Epoch: 1 [28800/60000 (48%)]\tLoss: -52650783118370114593412349952.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -57003822763347490594235613184.000000\n', 'Train Epoch: 1 [30080/60000 (50%)]\tLoss: -59303577461573144856143855616.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -59283479069822051646114365440.000000\n', 'Train Epoch: 1 [31360/60000 (52%)]\tLoss: -55061466205278376821390311424.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -59320785765036721843302563840.000000\n', 'Train Epoch: 1 [32640/60000 (54%)]\tLoss: -58551904221395334818544222208.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -58795814450235551993831620608.000000\n', 'Train Epoch: 1 [33920/60000 (57%)]\tLoss: -62447955739458932382392909824.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -64170953652032268265416818688.000000\n', 'Train Epoch: 1 [35200/60000 (59%)]\tLoss: -64199490912688249531443183616.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -65573165931790752018719571968.000000\n', 'Train Epoch: 1 [36480/60000 (61%)]\tLoss: -62385847175476230808542380032.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -66543088061340865579515379712.000000\n', 'Train Epoch: 1 [37760/60000 (63%)]\tLoss: -68046146477341030606356611072.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -68279733613049694737206870016.000000\n', 'Train Epoch: 1 [39040/60000 (65%)]\tLoss: -68392739842984765347170615296.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -66020718770471756904557182976.000000\n', 'Train Epoch: 1 [40320/60000 (67%)]\tLoss: -64910273181490891311137423360.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -70319843157314210165975678976.000000\n', 'Train Epoch: 1 [41600/60000 (69%)]\tLoss: -73260002696648299884958449664.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -73964093372144715377384882176.000000\n', 'Train Epoch: 1 [42880/60000 (71%)]\tLoss: -74609215857369537610027892736.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -70301680935821093510483804160.000000\n', 'Train Epoch: 1 [44160/60000 (74%)]\tLoss: -70990518367577840048869998592.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -78247865345551359422213652480.000000\n', 'Train Epoch: 1 [45440/60000 (76%)]\tLoss: -77075929822597048178466357248.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -74285894313753384480826982400.000000\n', 'Train Epoch: 1 [46720/60000 (78%)]\tLoss: -78705528770872188218453786624.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -74823653796990165329536614400.000000\n', 'Train Epoch: 1 [48000/60000 (80%)]\tLoss: -79730461747581250276343939072.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -83273823324871728387407740928.000000\n', 'Train Epoch: 1 [49280/60000 (82%)]\tLoss: -81781593295216783457041514496.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -78081288590234615556945739776.000000\n', 'Train Epoch: 1 [50560/60000 (84%)]\tLoss: -82117636894137787410448121856.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -81640007303327385754244481024.000000\n', 'Train Epoch: 1 [51840/60000 (86%)]\tLoss: -85267039769961348239204548608.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -83486329816600862422024060928.000000\n', 'Train Epoch: 1 [53120/60000 (88%)]\tLoss: -83382447198710695966613176320.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -88423214519321344139197415424.000000\n', 'Train Epoch: 1 [54400/60000 (91%)]\tLoss: -85895700085626886888632614912.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -90541091994825756885345370112.000000\n', 'Train Epoch: 1 [55680/60000 (93%)]\tLoss: -81870817687544122533709086720.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -85937020792351996284252454912.000000\n', 'Train Epoch: 1 [56960/60000 (95%)]\tLoss: -87649860894620679560421703680.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -88462523497924751065956220928.000000\n', 'Train Epoch: 1 [58240/60000 (97%)]\tLoss: -93880504008454060758920069120.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -90146387159454546199094231040.000000\n', 'Train Epoch: 1 [59520/60000 (99%)]\tLoss: -87638102202078334143839600640.000000\n', '\n', 'Test set: Average loss: -91465940632342573349200723968.0000, Accuracy: 980/10000 (10%)\n', '\n', '{"accuracy": 9.8, "runtime": 59.756867287214845, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}\n']
[2021-12-11 15:16:16,325 DEBUG] Received error from gpu1: []
[2021-12-11 15:16:16,326 DEBUG] SSH connection with gpu1 has been closed
[2021-12-11 15:16:16,326 INFO] {'batch-size': 64, 'lr': 0.99, 'gamma': 0.7} => {"accuracy": 9.8, "runtime": 59.756867287214845, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}
[2021-12-11 15:16:16,326 DEBUG] gpu1 finished training {'batch-size': 64, 'lr': 0.99, 'gamma': 0.7}
[2021-12-11 15:16:16,326 DEBUG] gpu1 now training {'batch-size': 128, 'lr': 0.5, 'gamma': 0.7}...
[2021-12-11 15:16:16,327 DEBUG] Attempting to establish SSH connection with gpu1...
[2021-12-11 15:16:16,410 DEBUG] SSH connection with gpu1 established successfully!
[2021-12-11 15:16:16,410 DEBUG] Running cmd on gpu1: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "128" --lr "0.5" --gamma "0.7"
[2021-12-11 15:16:16,411 INFO] Training {'batch-size': 128, 'lr': 0.5, 'gamma': 0.7} on gpu1...
[2021-12-11 15:16:18,854 DEBUG] Received output from gpu2: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002190\n', 'Train Epoch: 1 [640/60000 (1%)]\tLoss: -515892676818632704.000000\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -91063265233698850175188992.000000\n', 'Train Epoch: 1 [1920/60000 (3%)]\tLoss: -1487482723174087629838745600.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -3146413912987809522688983040.000000\n', 'Train Epoch: 1 [3200/60000 (5%)]\tLoss: -5008643422610927411018072064.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -6239527656075090077262282752.000000\n', 'Train Epoch: 1 [4480/60000 (7%)]\tLoss: -8094218202756211756360531968.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -9121254403437581882487734272.000000\n', 'Train Epoch: 1 [5760/60000 (10%)]\tLoss: -11323722449158017865937321984.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -12718703605239607475006603264.000000\n', 'Train Epoch: 1 [7040/60000 (12%)]\tLoss: -14619683968777438133900804096.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -15467662789823650281165946880.000000\n', 'Train Epoch: 1 [8320/60000 (14%)]\tLoss: -15895587112218607486672830464.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -18738464721413961796842160128.000000\n', 'Train Epoch: 1 [9600/60000 (16%)]\tLoss: -19821214811564417638494896128.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -21090098157228801263656763392.000000\n', 'Train Epoch: 1 [10880/60000 (18%)]\tLoss: -22768531097303296304799023104.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -23172508299263621539427254272.000000\n', 'Train Epoch: 1 [12160/60000 (20%)]\tLoss: -23747383221872516364693929984.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -26436914966044506838072819712.000000\n', 'Train Epoch: 1 [13440/60000 (22%)]\tLoss: -26916324889018950350522023936.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -28719043431372850149220941824.000000\n', 'Train Epoch: 1 [14720/60000 (25%)]\tLoss: -30206206361791675950272872448.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -31047054409448754933265334272.000000\n', 'Train Epoch: 1 [16000/60000 (27%)]\tLoss: -32035913228595175771368062976.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -31223009784600477913927647232.000000\n', 'Train Epoch: 1 [17280/60000 (29%)]\tLoss: -34667348059111662434505457664.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -33237604382758048649606856704.000000\n', 'Train Epoch: 1 [18560/60000 (31%)]\tLoss: -36686225843669195938393489408.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -38397979801646819196163588096.000000\n', 'Train Epoch: 1 [19840/60000 (33%)]\tLoss: -40130146188746646495369887744.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -44574506956769755695335604224.000000\n', 'Train Epoch: 1 [21120/60000 (35%)]\tLoss: -39802994805912886083900669952.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -43047789484690413745973755904.000000\n', 'Train Epoch: 1 [22400/60000 (37%)]\tLoss: -44911424193490090533106745344.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -45362032403285512079397617664.000000\n', 'Train Epoch: 1 [23680/60000 (39%)]\tLoss: -46470480431244123812891983872.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -46160570408439320476097970176.000000\n', 'Train Epoch: 1 [24960/60000 (42%)]\tLoss: -48130477252569495759119974400.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -50518984106474202133174419456.000000\n', 'Train Epoch: 1 [26240/60000 (44%)]\tLoss: -50461522351110644290214166528.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -51936699915395946931713736704.000000\n', 'Train Epoch: 1 [27520/60000 (46%)]\tLoss: -51554622687999929676764020736.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -57731917227676332493283262464.000000\n', 'Train Epoch: 1 [28800/60000 (48%)]\tLoss: -52650783118370114593412349952.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -57003822763347490594235613184.000000\n', 'Train Epoch: 1 [30080/60000 (50%)]\tLoss: -59303577461573144856143855616.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -59283479069822051646114365440.000000\n', 'Train Epoch: 1 [31360/60000 (52%)]\tLoss: -55061466205278376821390311424.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -59320785765036721843302563840.000000\n', 'Train Epoch: 1 [32640/60000 (54%)]\tLoss: -58551904221395334818544222208.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -58795814450235551993831620608.000000\n', 'Train Epoch: 1 [33920/60000 (57%)]\tLoss: -62447955739458932382392909824.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -64170953652032268265416818688.000000\n', 'Train Epoch: 1 [35200/60000 (59%)]\tLoss: -64199490912688249531443183616.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -65573165931790752018719571968.000000\n', 'Train Epoch: 1 [36480/60000 (61%)]\tLoss: -62385847175476230808542380032.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -66543088061340865579515379712.000000\n', 'Train Epoch: 1 [37760/60000 (63%)]\tLoss: -68046146477341030606356611072.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -68279733613049694737206870016.000000\n', 'Train Epoch: 1 [39040/60000 (65%)]\tLoss: -68392739842984765347170615296.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -66020718770471756904557182976.000000\n', 'Train Epoch: 1 [40320/60000 (67%)]\tLoss: -64910273181490891311137423360.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -70319843157314210165975678976.000000\n', 'Train Epoch: 1 [41600/60000 (69%)]\tLoss: -73260002696648299884958449664.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -73964093372144715377384882176.000000\n', 'Train Epoch: 1 [42880/60000 (71%)]\tLoss: -74609215857369537610027892736.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -70301680935821093510483804160.000000\n', 'Train Epoch: 1 [44160/60000 (74%)]\tLoss: -70990518367577840048869998592.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -78247865345551359422213652480.000000\n', 'Train Epoch: 1 [45440/60000 (76%)]\tLoss: -77075929822597048178466357248.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -74285894313753384480826982400.000000\n', 'Train Epoch: 1 [46720/60000 (78%)]\tLoss: -78705528770872188218453786624.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -74823653796990165329536614400.000000\n', 'Train Epoch: 1 [48000/60000 (80%)]\tLoss: -79730461747581250276343939072.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -83273823324871728387407740928.000000\n', 'Train Epoch: 1 [49280/60000 (82%)]\tLoss: -81781593295216783457041514496.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -78081288590234615556945739776.000000\n', 'Train Epoch: 1 [50560/60000 (84%)]\tLoss: -82117636894137787410448121856.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -81640007303327385754244481024.000000\n', 'Train Epoch: 1 [51840/60000 (86%)]\tLoss: -85267039769961348239204548608.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -83486329816600862422024060928.000000\n', 'Train Epoch: 1 [53120/60000 (88%)]\tLoss: -83382447198710695966613176320.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -88423214519321344139197415424.000000\n', 'Train Epoch: 1 [54400/60000 (91%)]\tLoss: -85895700085626886888632614912.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -90541091994825756885345370112.000000\n', 'Train Epoch: 1 [55680/60000 (93%)]\tLoss: -81870817687544122533709086720.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -85937020792351996284252454912.000000\n', 'Train Epoch: 1 [56960/60000 (95%)]\tLoss: -87649860894620679560421703680.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -88462523497924751065956220928.000000\n', 'Train Epoch: 1 [58240/60000 (97%)]\tLoss: -93880504008454060758920069120.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -90146387159454546199094231040.000000\n', 'Train Epoch: 1 [59520/60000 (99%)]\tLoss: -87638102202078334143839600640.000000\n', '\n', 'Test set: Average loss: -91465940632342573349200723968.0000, Accuracy: 980/10000 (10%)\n', '\n', '{"accuracy": 9.8, "runtime": 62.047039307653904, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}\n']
[2021-12-11 15:16:18,854 DEBUG] Received error from gpu2: []
[2021-12-11 15:16:18,855 DEBUG] SSH connection with gpu2 has been closed
[2021-12-11 15:16:18,855 INFO] {'batch-size': 64, 'lr': 0.99, 'gamma': 0.9} => {"accuracy": 9.8, "runtime": 62.047039307653904, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2156343296}
[2021-12-11 15:16:18,855 DEBUG] gpu2 finished training {'batch-size': 64, 'lr': 0.99, 'gamma': 0.9}
[2021-12-11 15:16:18,855 DEBUG] gpu2 now training {'batch-size': 128, 'lr': 0.5, 'gamma': 0.9}...
[2021-12-11 15:16:18,855 DEBUG] Attempting to establish SSH connection with gpu2...
[2021-12-11 15:16:18,936 DEBUG] SSH connection with gpu2 established successfully!
[2021-12-11 15:16:18,937 DEBUG] Running cmd on gpu2: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "128" --lr "0.5" --gamma "0.9"
[2021-12-11 15:16:18,937 INFO] Training {'batch-size': 128, 'lr': 0.5, 'gamma': 0.9} on gpu2...
[2021-12-11 15:17:15,366 DEBUG] Received output from gpu3: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.001658\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -291611099529216.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -17900646662151454049959936.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -1040649012364052237161332736.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -3044006739475634652054552576.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -4903260567918734305193361408.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -6542085544561875735164223488.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -8628987837713525760944242688.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -9957092020256336171908464640.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -12194540995698629052278505472.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -14272626630626761450444554240.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -15030046990814223846269255680.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -17651292135052298532446797824.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -19130017377157856994913157120.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -21297336849146294209037205504.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -23436502752755483315808501760.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -25162036576130120198312165376.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -27567180327153976332454461440.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -28380322250656059106978168832.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -29876608793119789062582960128.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -31476017652469943460719230976.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -33553592091226305219790897152.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -33788329123173548109250691072.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -36766659550788673170495766528.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -39508930683841351841621213184.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -39397711869620047392370851840.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -41996236528117415498013999104.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -44746103587657789993465675776.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -45080942983125662187342790656.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -45973616641748994091732959232.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -47120415007914990214007750656.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -50529897495416113883263270912.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -49885558923027448011725733888.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -52016762916746518896666738688.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -54507263441948243868046589952.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -57516610372622856759055220736.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -58428215998476013071107096576.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -60453313139691485897741565952.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -63036839562239260010186014720.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -60661588391051968730246414336.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -62710779046063042486761160704.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -65237042052639539281344331776.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -67229144019239201896870707200.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -69378699129110704223112134656.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -70839128742870004962609332224.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -71305287705493516250588905472.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -72133501061625677497812123648.000000\n', '\n', 'Test set: Average loss: -75628971291913073481463365632.0000, Accuracy: 1009/10000 (10%)\n', '\n', '{"accuracy": 10.09, "runtime": 57.63888473622501, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}\n']
[2021-12-11 15:17:15,367 DEBUG] Received error from gpu3: []
[2021-12-11 15:17:15,367 DEBUG] SSH connection with gpu3 has been closed
[2021-12-11 15:17:15,367 INFO] {'batch-size': 128, 'lr': 0.5, 'gamma': 0.5} => {"accuracy": 10.09, "runtime": 57.63888473622501, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}
[2021-12-11 15:17:15,368 DEBUG] gpu3 finished training {'batch-size': 128, 'lr': 0.5, 'gamma': 0.5}
[2021-12-11 15:17:15,368 DEBUG] gpu3 now training {'batch-size': 128, 'lr': 0.9, 'gamma': 0.5}...
[2021-12-11 15:17:15,368 DEBUG] Attempting to establish SSH connection with gpu3...
[2021-12-11 15:17:15,463 DEBUG] SSH connection with gpu3 established successfully!
[2021-12-11 15:17:15,463 DEBUG] Running cmd on gpu3: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "128" --lr "0.9" --gamma "0.5"
[2021-12-11 15:17:15,463 INFO] Training {'batch-size': 128, 'lr': 0.9, 'gamma': 0.5} on gpu3...
[2021-12-11 15:17:16,744 DEBUG] Received output from gpu1: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.001658\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -291611099529216.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -17900646662151454049959936.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -1040649012364052237161332736.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -3044006739475634652054552576.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -4903260567918734305193361408.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -6542085544561875735164223488.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -8628987837713525760944242688.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -9957092020256336171908464640.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -12194540995698629052278505472.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -14272626630626761450444554240.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -15030046990814223846269255680.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -17651292135052298532446797824.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -19130017377157856994913157120.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -21297336849146294209037205504.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -23436502752755483315808501760.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -25162036576130120198312165376.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -27567180327153976332454461440.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -28380322250656059106978168832.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -29876608793119789062582960128.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -31476017652469943460719230976.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -33553592091226305219790897152.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -33788329123173548109250691072.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -36766659550788673170495766528.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -39508930683841351841621213184.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -39397711869620047392370851840.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -41996236528117415498013999104.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -44746103587657789993465675776.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -45080942983125662187342790656.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -45973616641748994091732959232.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -47120415007914990214007750656.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -50529897495416113883263270912.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -49885558923027448011725733888.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -52016762916746518896666738688.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -54507263441948243868046589952.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -57516610372622856759055220736.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -58428215998476013071107096576.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -60453313139691485897741565952.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -63036839562239260010186014720.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -60661588391051968730246414336.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -62710779046063042486761160704.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -65237042052639539281344331776.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -67229144019239201896870707200.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -69378699129110704223112134656.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -70839128742870004962609332224.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -71305287705493516250588905472.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -72133501061625677497812123648.000000\n', '\n', 'Test set: Average loss: -75628971291913073481463365632.0000, Accuracy: 1009/10000 (10%)\n', '\n', '{"accuracy": 10.09, "runtime": 57.32076796237379, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}\n']
[2021-12-11 15:17:16,745 DEBUG] Received error from gpu1: []
[2021-12-11 15:17:16,745 DEBUG] SSH connection with gpu1 has been closed
[2021-12-11 15:17:16,745 INFO] {'batch-size': 128, 'lr': 0.5, 'gamma': 0.7} => {"accuracy": 10.09, "runtime": 57.32076796237379, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}
[2021-12-11 15:17:16,746 DEBUG] gpu1 finished training {'batch-size': 128, 'lr': 0.5, 'gamma': 0.7}
[2021-12-11 15:17:16,746 DEBUG] gpu1 now training {'batch-size': 128, 'lr': 0.9, 'gamma': 0.7}...
[2021-12-11 15:17:16,746 DEBUG] Attempting to establish SSH connection with gpu1...
[2021-12-11 15:17:16,843 DEBUG] SSH connection with gpu1 established successfully!
[2021-12-11 15:17:16,843 DEBUG] Running cmd on gpu1: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "128" --lr "0.9" --gamma "0.7"
[2021-12-11 15:17:16,843 INFO] Training {'batch-size': 128, 'lr': 0.9, 'gamma': 0.7} on gpu1...
[2021-12-11 15:17:20,382 DEBUG] Received output from gpu2: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.001658\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -291611099529216.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -17900646662151454049959936.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -1040649012364052237161332736.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -3044006739475634652054552576.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -4903260567918734305193361408.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -6542085544561875735164223488.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -8628987837713525760944242688.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -9957092020256336171908464640.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -12194540995698629052278505472.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -14272626630626761450444554240.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -15030046990814223846269255680.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -17651292135052298532446797824.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -19130017377157856994913157120.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -21297336849146294209037205504.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -23436502752755483315808501760.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -25162036576130120198312165376.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -27567180327153976332454461440.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -28380322250656059106978168832.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -29876608793119789062582960128.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -31476017652469943460719230976.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -33553592091226305219790897152.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -33788329123173548109250691072.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -36766659550788673170495766528.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -39508930683841351841621213184.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -39397711869620047392370851840.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -41996236528117415498013999104.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -44746103587657789993465675776.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -45080942983125662187342790656.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -45973616641748994091732959232.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -47120415007914990214007750656.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -50529897495416113883263270912.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -49885558923027448011725733888.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -52016762916746518896666738688.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -54507263441948243868046589952.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -57516610372622856759055220736.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -58428215998476013071107096576.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -60453313139691485897741565952.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -63036839562239260010186014720.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -60661588391051968730246414336.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -62710779046063042486761160704.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -65237042052639539281344331776.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -67229144019239201896870707200.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -69378699129110704223112134656.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -70839128742870004962609332224.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -71305287705493516250588905472.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -72133501061625677497812123648.000000\n', '\n', 'Test set: Average loss: -75628971291913073481463365632.0000, Accuracy: 1009/10000 (10%)\n', '\n', '{"accuracy": 10.09, "runtime": 58.421860055066645, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}\n']
[2021-12-11 15:17:20,382 DEBUG] Received error from gpu2: []
[2021-12-11 15:17:20,383 DEBUG] SSH connection with gpu2 has been closed
[2021-12-11 15:17:20,383 INFO] {'batch-size': 128, 'lr': 0.5, 'gamma': 0.9} => {"accuracy": 10.09, "runtime": 58.421860055066645, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}
[2021-12-11 15:17:20,383 DEBUG] gpu2 finished training {'batch-size': 128, 'lr': 0.5, 'gamma': 0.9}
[2021-12-11 15:17:20,384 DEBUG] gpu2 now training {'batch-size': 128, 'lr': 0.9, 'gamma': 0.9}...
[2021-12-11 15:17:20,384 DEBUG] Attempting to establish SSH connection with gpu2...
[2021-12-11 15:17:20,475 DEBUG] SSH connection with gpu2 established successfully!
[2021-12-11 15:17:20,476 DEBUG] Running cmd on gpu2: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "128" --lr "0.9" --gamma "0.9"
[2021-12-11 15:17:20,476 INFO] Training {'batch-size': 128, 'lr': 0.9, 'gamma': 0.9} on gpu2...
[2021-12-11 15:18:15,981 DEBUG] Received output from gpu3: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.001658\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -194163308104777728.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -60899582822854723547168768.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -1642235410687488851873628160.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -4118946625759234459229487104.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -6538403279296858129308844032.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -8673663785824714039488413696.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -11410107518637531568420159488.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -13267531134920614054685835264.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -15873899644146028641028931584.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -18324280124300913824084525056.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -20281525123298894870877306880.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -22343256022505228970257022976.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -24498137961769408256299499520.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -27173434132178788184181702656.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -29811638475048468483732930560.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -32362743490508101046962749440.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -34751852446139373300781940736.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -36197378271278737439984713728.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -38378818799642575610709016576.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -40364531404390915596261261312.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -43084033647446438272988872704.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -44245948308724100129592705024.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -47107877124902971305965387776.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -50240813108800765681861656576.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -50965233572005934996933050368.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -53275183859468104822857662464.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -56890896813642628768240500736.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -58462358708147160606002118656.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -59566958007422233578647322624.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -61487563345471249765638340608.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -64642869179398397650911887360.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -64591178155876906514402770944.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -66781076442611564219704803328.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -69874160375760421659642691584.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -74213047088388637333760507904.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -73971181644235502714850639872.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -76994945959782316632696684544.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -81351763497945988349690904576.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -78205179874912700699127054336.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -81850577624798543234323185664.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -84240491743915144762651312128.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -86023213425397333048692834304.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -90557629722248766382883733504.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -90373391316286090044516597760.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -91084249142952457738534256640.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -93436866011587354808964612096.000000\n', '\n', 'Test set: Average loss: -96906394061383154331797159936.0000, Accuracy: 980/10000 (10%)\n', '\n', '{"accuracy": 9.8, "runtime": 58.477564914152026, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}\n']
[2021-12-11 15:18:15,981 DEBUG] Received error from gpu3: []
[2021-12-11 15:18:15,981 DEBUG] SSH connection with gpu3 has been closed
[2021-12-11 15:18:15,981 INFO] {'batch-size': 128, 'lr': 0.9, 'gamma': 0.5} => {"accuracy": 9.8, "runtime": 58.477564914152026, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}
[2021-12-11 15:18:15,982 DEBUG] gpu3 finished training {'batch-size': 128, 'lr': 0.9, 'gamma': 0.5}
[2021-12-11 15:18:15,982 DEBUG] gpu3 now training {'batch-size': 128, 'lr': 0.99, 'gamma': 0.5}...
[2021-12-11 15:18:15,982 DEBUG] Attempting to establish SSH connection with gpu3...
[2021-12-11 15:18:16,097 DEBUG] SSH connection with gpu3 established successfully!
[2021-12-11 15:18:16,097 DEBUG] Running cmd on gpu3: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "128" --lr "0.99" --gamma "0.5"
[2021-12-11 15:18:16,097 INFO] Training {'batch-size': 128, 'lr': 0.99, 'gamma': 0.5} on gpu3...
[2021-12-11 15:18:17,490 DEBUG] Received output from gpu1: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.001658\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -194163308104777728.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -60899582822854723547168768.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -1642235410687488851873628160.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -4118946625759234459229487104.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -6538403279296858129308844032.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -8673663785824714039488413696.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -11410107518637531568420159488.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -13267531134920614054685835264.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -15873899644146028641028931584.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -18324280124300913824084525056.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -20281525123298894870877306880.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -22343256022505228970257022976.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -24498137961769408256299499520.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -27173434132178788184181702656.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -29811638475048468483732930560.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -32362743490508101046962749440.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -34751852446139373300781940736.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -36197378271278737439984713728.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -38378818799642575610709016576.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -40364531404390915596261261312.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -43084033647446438272988872704.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -44245948308724100129592705024.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -47107877124902971305965387776.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -50240813108800765681861656576.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -50965233572005934996933050368.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -53275183859468104822857662464.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -56890896813642628768240500736.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -58462358708147160606002118656.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -59566958007422233578647322624.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -61487563345471249765638340608.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -64642869179398397650911887360.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -64591178155876906514402770944.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -66781076442611564219704803328.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -69874160375760421659642691584.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -74213047088388637333760507904.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -73971181644235502714850639872.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -76994945959782316632696684544.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -81351763497945988349690904576.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -78205179874912700699127054336.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -81850577624798543234323185664.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -84240491743915144762651312128.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -86023213425397333048692834304.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -90557629722248766382883733504.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -90373391316286090044516597760.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -91084249142952457738534256640.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -93436866011587354808964612096.000000\n', '\n', 'Test set: Average loss: -96906394061383154331797159936.0000, Accuracy: 980/10000 (10%)\n', '\n', '{"accuracy": 9.8, "runtime": 57.65384897682816, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}\n']
[2021-12-11 15:18:17,490 DEBUG] Received error from gpu1: []
[2021-12-11 15:18:17,490 DEBUG] SSH connection with gpu1 has been closed
[2021-12-11 15:18:17,490 INFO] {'batch-size': 128, 'lr': 0.9, 'gamma': 0.7} => {"accuracy": 9.8, "runtime": 57.65384897682816, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}
[2021-12-11 15:18:17,491 DEBUG] gpu1 finished training {'batch-size': 128, 'lr': 0.9, 'gamma': 0.7}
[2021-12-11 15:18:17,491 DEBUG] gpu1 now training {'batch-size': 128, 'lr': 0.99, 'gamma': 0.7}...
[2021-12-11 15:18:17,491 DEBUG] Attempting to establish SSH connection with gpu1...
[2021-12-11 15:18:17,595 DEBUG] SSH connection with gpu1 established successfully!
[2021-12-11 15:18:17,596 DEBUG] Running cmd on gpu1: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "128" --lr "0.99" --gamma "0.7"
[2021-12-11 15:18:17,596 INFO] Training {'batch-size': 128, 'lr': 0.99, 'gamma': 0.7} on gpu1...
[2021-12-11 15:18:22,158 DEBUG] Received output from gpu2: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.001658\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -194163308104777728.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -60899582822854723547168768.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -1642235410687488851873628160.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -4118946625759234459229487104.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -6538403279296858129308844032.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -8673663785824714039488413696.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -11410107518637531568420159488.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -13267531134920614054685835264.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -15873899644146028641028931584.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -18324280124300913824084525056.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -20281525123298894870877306880.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -22343256022505228970257022976.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -24498137961769408256299499520.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -27173434132178788184181702656.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -29811638475048468483732930560.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -32362743490508101046962749440.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -34751852446139373300781940736.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -36197378271278737439984713728.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -38378818799642575610709016576.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -40364531404390915596261261312.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -43084033647446438272988872704.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -44245948308724100129592705024.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -47107877124902971305965387776.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -50240813108800765681861656576.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -50965233572005934996933050368.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -53275183859468104822857662464.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -56890896813642628768240500736.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -58462358708147160606002118656.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -59566958007422233578647322624.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -61487563345471249765638340608.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -64642869179398397650911887360.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -64591178155876906514402770944.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -66781076442611564219704803328.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -69874160375760421659642691584.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -74213047088388637333760507904.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -73971181644235502714850639872.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -76994945959782316632696684544.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -81351763497945988349690904576.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -78205179874912700699127054336.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -81850577624798543234323185664.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -84240491743915144762651312128.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -86023213425397333048692834304.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -90557629722248766382883733504.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -90373391316286090044516597760.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -91084249142952457738534256640.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -93436866011587354808964612096.000000\n', '\n', 'Test set: Average loss: -96906394061383154331797159936.0000, Accuracy: 980/10000 (10%)\n', '\n', '{"accuracy": 9.8, "runtime": 58.678995272144675, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}\n']
[2021-12-11 15:18:22,159 DEBUG] Received error from gpu2: []
[2021-12-11 15:18:22,159 DEBUG] SSH connection with gpu2 has been closed
[2021-12-11 15:18:22,159 INFO] {'batch-size': 128, 'lr': 0.9, 'gamma': 0.9} => {"accuracy": 9.8, "runtime": 58.678995272144675, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}
[2021-12-11 15:18:22,160 DEBUG] gpu2 finished training {'batch-size': 128, 'lr': 0.9, 'gamma': 0.9}
[2021-12-11 15:18:22,160 DEBUG] gpu2 now training {'batch-size': 128, 'lr': 0.99, 'gamma': 0.9}...
[2021-12-11 15:18:22,160 DEBUG] Attempting to establish SSH connection with gpu2...
[2021-12-11 15:18:24,579 DEBUG] SSH connection with gpu2 established successfully!
[2021-12-11 15:18:24,579 DEBUG] Running cmd on gpu2: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "128" --lr "0.99" --gamma "0.9"
[2021-12-11 15:18:24,579 INFO] Training {'batch-size': 128, 'lr': 0.99, 'gamma': 0.9} on gpu2...
[2021-12-11 15:19:15,908 DEBUG] Received output from gpu3: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.001658\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -498038325970468864.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -89033523866408403937001472.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -1697338343992853307049639936.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -3611935946431943767270359040.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -5531043999157985097375809536.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -7326921603365637856277037056.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -9529409128847815677013131264.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -11025029227341650653937139712.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -12948146864359553492181843968.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -14921692291864779836841000960.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -17278610055773950436649730048.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -18408707772873278058626285568.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -20042327816208581601513177088.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -22135198000002958141432004608.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -24615290429476438414760869888.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -26323573448089152483298902016.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -28283544433139370032791289856.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -29197062617418088551154712576.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -32136877423998928786036883456.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -32809757979410058793245999104.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -35526847093192835081269411840.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -36805361705299031347844612096.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -38702206456389488914942918656.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -40844371062715300246424911872.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -41935436059650468815887663104.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -43132447348628817875719684096.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -46210769164152224804719558656.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -47289522951666552209110204416.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -48014991780230918585419038720.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -50270861526731265234356404224.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -51790948796268658201838223360.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -53713811425496486079241388032.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -55207557334792432165721210880.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -57110283793337303875933175808.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -60479904785356524869939888128.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -58822311648570933573125668864.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -63241313308581032778293837824.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -65850321620670371496311390208.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -63609780675773419715737681920.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -66238377364033701722101645312.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -68361354995339613685080391680.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -69896832457244678826313646080.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -72724410779627156203401904128.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -73160620494016308201436217344.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -74376384300298132622411825152.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -74488526337166838087301464064.000000\n', '\n', 'Test set: Average loss: -78551986608407807981321715712.0000, Accuracy: 980/10000 (10%)\n', '\n', '{"accuracy": 9.8, "runtime": 57.83978778868914, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}\n']
[2021-12-11 15:19:15,908 DEBUG] Received error from gpu3: []
[2021-12-11 15:19:15,909 DEBUG] SSH connection with gpu3 has been closed
[2021-12-11 15:19:15,909 INFO] {'batch-size': 128, 'lr': 0.99, 'gamma': 0.5} => {"accuracy": 9.8, "runtime": 57.83978778868914, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}
[2021-12-11 15:19:15,910 DEBUG] gpu3 finished training {'batch-size': 128, 'lr': 0.99, 'gamma': 0.5}
[2021-12-11 15:19:15,910 DEBUG] gpu3 now training {'batch-size': 256, 'lr': 0.5, 'gamma': 0.5}...
[2021-12-11 15:19:15,910 DEBUG] Attempting to establish SSH connection with gpu3...
[2021-12-11 15:19:16,019 DEBUG] SSH connection with gpu3 established successfully!
[2021-12-11 15:19:16,019 DEBUG] Running cmd on gpu3: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "256" --lr "0.5" --gamma "0.5"
[2021-12-11 15:19:16,020 INFO] Training {'batch-size': 256, 'lr': 0.5, 'gamma': 0.5} on gpu3...
[2021-12-11 15:19:19,538 DEBUG] Received output from gpu1: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.001658\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -498038325970468864.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -89033523866408403937001472.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -1697338343992853307049639936.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -3611935946431943767270359040.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -5531043999157985097375809536.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -7326921603365637856277037056.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -9529409128847815677013131264.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -11025029227341650653937139712.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -12948146864359553492181843968.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -14921692291864779836841000960.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -17278610055773950436649730048.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -18408707772873278058626285568.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -20042327816208581601513177088.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -22135198000002958141432004608.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -24615290429476438414760869888.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -26323573448089152483298902016.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -28283544433139370032791289856.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -29197062617418088551154712576.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -32136877423998928786036883456.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -32809757979410058793245999104.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -35526847093192835081269411840.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -36805361705299031347844612096.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -38702206456389488914942918656.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -40844371062715300246424911872.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -41935436059650468815887663104.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -43132447348628817875719684096.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -46210769164152224804719558656.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -47289522951666552209110204416.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -48014991780230918585419038720.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -50270861526731265234356404224.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -51790948796268658201838223360.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -53713811425496486079241388032.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -55207557334792432165721210880.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -57110283793337303875933175808.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -60479904785356524869939888128.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -58822311648570933573125668864.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -63241313308581032778293837824.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -65850321620670371496311390208.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -63609780675773419715737681920.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -66238377364033701722101645312.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -68361354995339613685080391680.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -69896832457244678826313646080.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -72724410779627156203401904128.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -73160620494016308201436217344.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -74376384300298132622411825152.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -74488526337166838087301464064.000000\n', '\n', 'Test set: Average loss: -78551986608407807981321715712.0000, Accuracy: 980/10000 (10%)\n', '\n', '{"accuracy": 9.8, "runtime": 58.865964769851416, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}\n']
[2021-12-11 15:19:19,538 DEBUG] Received error from gpu1: []
[2021-12-11 15:19:19,539 DEBUG] SSH connection with gpu1 has been closed
[2021-12-11 15:19:19,539 INFO] {'batch-size': 128, 'lr': 0.99, 'gamma': 0.7} => {"accuracy": 9.8, "runtime": 58.865964769851416, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}
[2021-12-11 15:19:19,539 DEBUG] gpu1 finished training {'batch-size': 128, 'lr': 0.99, 'gamma': 0.7}
[2021-12-11 15:19:19,540 DEBUG] gpu1 now training {'batch-size': 256, 'lr': 0.5, 'gamma': 0.7}...
[2021-12-11 15:19:19,540 DEBUG] Attempting to establish SSH connection with gpu1...
[2021-12-11 15:19:19,629 DEBUG] SSH connection with gpu1 established successfully!
[2021-12-11 15:19:19,629 DEBUG] Running cmd on gpu1: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "256" --lr "0.5" --gamma "0.7"
[2021-12-11 15:19:19,629 INFO] Training {'batch-size': 256, 'lr': 0.5, 'gamma': 0.7} on gpu1...
[2021-12-11 15:19:26,231 DEBUG] Received output from gpu2: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.001658\n', 'Train Epoch: 1 [1280/60000 (2%)]\tLoss: -498038325970468864.000000\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -89033523866408403937001472.000000\n', 'Train Epoch: 1 [3840/60000 (6%)]\tLoss: -1697338343992853307049639936.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -3611935946431943767270359040.000000\n', 'Train Epoch: 1 [6400/60000 (11%)]\tLoss: -5531043999157985097375809536.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -7326921603365637856277037056.000000\n', 'Train Epoch: 1 [8960/60000 (15%)]\tLoss: -9529409128847815677013131264.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -11025029227341650653937139712.000000\n', 'Train Epoch: 1 [11520/60000 (19%)]\tLoss: -12948146864359553492181843968.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -14921692291864779836841000960.000000\n', 'Train Epoch: 1 [14080/60000 (23%)]\tLoss: -17278610055773950436649730048.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -18408707772873278058626285568.000000\n', 'Train Epoch: 1 [16640/60000 (28%)]\tLoss: -20042327816208581601513177088.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -22135198000002958141432004608.000000\n', 'Train Epoch: 1 [19200/60000 (32%)]\tLoss: -24615290429476438414760869888.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -26323573448089152483298902016.000000\n', 'Train Epoch: 1 [21760/60000 (36%)]\tLoss: -28283544433139370032791289856.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -29197062617418088551154712576.000000\n', 'Train Epoch: 1 [24320/60000 (41%)]\tLoss: -32136877423998928786036883456.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -32809757979410058793245999104.000000\n', 'Train Epoch: 1 [26880/60000 (45%)]\tLoss: -35526847093192835081269411840.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -36805361705299031347844612096.000000\n', 'Train Epoch: 1 [29440/60000 (49%)]\tLoss: -38702206456389488914942918656.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -40844371062715300246424911872.000000\n', 'Train Epoch: 1 [32000/60000 (53%)]\tLoss: -41935436059650468815887663104.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -43132447348628817875719684096.000000\n', 'Train Epoch: 1 [34560/60000 (58%)]\tLoss: -46210769164152224804719558656.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -47289522951666552209110204416.000000\n', 'Train Epoch: 1 [37120/60000 (62%)]\tLoss: -48014991780230918585419038720.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -50270861526731265234356404224.000000\n', 'Train Epoch: 1 [39680/60000 (66%)]\tLoss: -51790948796268658201838223360.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -53713811425496486079241388032.000000\n', 'Train Epoch: 1 [42240/60000 (70%)]\tLoss: -55207557334792432165721210880.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -57110283793337303875933175808.000000\n', 'Train Epoch: 1 [44800/60000 (75%)]\tLoss: -60479904785356524869939888128.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -58822311648570933573125668864.000000\n', 'Train Epoch: 1 [47360/60000 (79%)]\tLoss: -63241313308581032778293837824.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -65850321620670371496311390208.000000\n', 'Train Epoch: 1 [49920/60000 (83%)]\tLoss: -63609780675773419715737681920.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -66238377364033701722101645312.000000\n', 'Train Epoch: 1 [52480/60000 (87%)]\tLoss: -68361354995339613685080391680.000000\n', 'Train Epoch: 1 [53760/60000 (90%)]\tLoss: -69896832457244678826313646080.000000\n', 'Train Epoch: 1 [55040/60000 (92%)]\tLoss: -72724410779627156203401904128.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -73160620494016308201436217344.000000\n', 'Train Epoch: 1 [57600/60000 (96%)]\tLoss: -74376384300298132622411825152.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -74488526337166838087301464064.000000\n', '\n', 'Test set: Average loss: -78551986608407807981321715712.0000, Accuracy: 980/10000 (10%)\n', '\n', '{"accuracy": 9.8, "runtime": 57.36283093178645, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}\n']
[2021-12-11 15:19:26,231 DEBUG] Received error from gpu2: []
[2021-12-11 15:19:26,232 DEBUG] SSH connection with gpu2 has been closed
[2021-12-11 15:19:26,232 INFO] {'batch-size': 128, 'lr': 0.99, 'gamma': 0.9} => {"accuracy": 9.8, "runtime": 57.36283093178645, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154813440}
[2021-12-11 15:19:26,233 DEBUG] gpu2 finished training {'batch-size': 128, 'lr': 0.99, 'gamma': 0.9}
[2021-12-11 15:19:26,233 DEBUG] gpu2 now training {'batch-size': 256, 'lr': 0.5, 'gamma': 0.9}...
[2021-12-11 15:19:26,233 DEBUG] Attempting to establish SSH connection with gpu2...
[2021-12-11 15:19:26,342 DEBUG] SSH connection with gpu2 established successfully!
[2021-12-11 15:19:26,342 DEBUG] Running cmd on gpu2: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "256" --lr "0.5" --gamma "0.9"
[2021-12-11 15:19:26,342 INFO] Training {'batch-size': 256, 'lr': 0.5, 'gamma': 0.9} on gpu2...
[2021-12-11 15:20:15,627 DEBUG] Received output from gpu3: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002335\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -304248201936896.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -18976280920775476432076800.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -1103606643082978514253643776.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -3209556380082354369488486400.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -5281567871121895686498418688.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -7137097228149537086327750656.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -9198847016821802664288321536.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -11180658356559481964188401664.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -13330800220466480843847630848.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -15026084925335096213934964736.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -16988620155385511980778389504.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -19356654369995357725420158976.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -21280442583053828053285208064.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -23324755033488097466294206464.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -25276502017308397528646942720.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -26747382228094288793002049536.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -29545832994010426198412230656.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -30947348724712686679045963776.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -33798909585278417549351976960.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -34977236831267292857896009728.000000\n', 'Train Epoch: 1 [53760/60000 (89%)]\tLoss: -37288570772108943489868234752.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -38863024285780373247872729088.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -41103397586667183156041351168.000000\n', '\n', 'Test set: Average loss: -42441783411297508080736010240.0000, Accuracy: 1135/10000 (11%)\n', '\n', '{"accuracy": 11.35, "runtime": 57.599145675078034, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}\n']
[2021-12-11 15:20:15,628 DEBUG] Received error from gpu3: []
[2021-12-11 15:20:15,628 DEBUG] SSH connection with gpu3 has been closed
[2021-12-11 15:20:15,628 INFO] {'batch-size': 256, 'lr': 0.5, 'gamma': 0.5} => {"accuracy": 11.35, "runtime": 57.599145675078034, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}
[2021-12-11 15:20:15,629 DEBUG] gpu3 finished training {'batch-size': 256, 'lr': 0.5, 'gamma': 0.5}
[2021-12-11 15:20:15,629 DEBUG] gpu3 now training {'batch-size': 256, 'lr': 0.9, 'gamma': 0.5}...
[2021-12-11 15:20:15,629 DEBUG] Attempting to establish SSH connection with gpu3...
[2021-12-11 15:20:15,726 DEBUG] SSH connection with gpu3 established successfully!
[2021-12-11 15:20:15,726 DEBUG] Running cmd on gpu3: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "256" --lr "0.9" --gamma "0.5"
[2021-12-11 15:20:15,726 INFO] Training {'batch-size': 256, 'lr': 0.9, 'gamma': 0.5} on gpu3...
[2021-12-11 15:20:19,282 DEBUG] Received output from gpu1: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002335\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -304248201936896.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -18976280920775476432076800.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -1103606643082978514253643776.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -3209556380082354369488486400.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -5281567871121895686498418688.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -7137097228149537086327750656.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -9198847016821802664288321536.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -11180658356559481964188401664.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -13330800220466480843847630848.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -15026084925335096213934964736.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -16988620155385511980778389504.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -19356654369995357725420158976.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -21280442583053828053285208064.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -23324755033488097466294206464.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -25276502017308397528646942720.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -26747382228094288793002049536.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -29545832994010426198412230656.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -30947348724712686679045963776.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -33798909585278417549351976960.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -34977236831267292857896009728.000000\n', 'Train Epoch: 1 [53760/60000 (89%)]\tLoss: -37288570772108943489868234752.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -38863024285780373247872729088.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -41103397586667183156041351168.000000\n', '\n', 'Test set: Average loss: -42441783411297508080736010240.0000, Accuracy: 1135/10000 (11%)\n', '\n', '{"accuracy": 11.35, "runtime": 56.62067863531411, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}\n']
[2021-12-11 15:20:19,282 DEBUG] Received error from gpu1: []
[2021-12-11 15:20:19,282 DEBUG] SSH connection with gpu1 has been closed
[2021-12-11 15:20:19,282 INFO] {'batch-size': 256, 'lr': 0.5, 'gamma': 0.7} => {"accuracy": 11.35, "runtime": 56.62067863531411, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}
[2021-12-11 15:20:19,283 DEBUG] gpu1 finished training {'batch-size': 256, 'lr': 0.5, 'gamma': 0.7}
[2021-12-11 15:20:19,283 DEBUG] gpu1 now training {'batch-size': 256, 'lr': 0.9, 'gamma': 0.7}...
[2021-12-11 15:20:19,283 DEBUG] Attempting to establish SSH connection with gpu1...
[2021-12-11 15:20:19,372 DEBUG] SSH connection with gpu1 established successfully!
[2021-12-11 15:20:19,373 DEBUG] Running cmd on gpu1: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "256" --lr "0.9" --gamma "0.7"
[2021-12-11 15:20:19,373 INFO] Training {'batch-size': 256, 'lr': 0.9, 'gamma': 0.7} on gpu1...
[2021-12-11 15:20:29,379 DEBUG] Received output from gpu2: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002335\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -304248201936896.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -18976280920775476432076800.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -1103606643082978514253643776.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -3209556380082354369488486400.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -5281567871121895686498418688.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -7137097228149537086327750656.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -9198847016821802664288321536.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -11180658356559481964188401664.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -13330800220466480843847630848.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -15026084925335096213934964736.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -16988620155385511980778389504.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -19356654369995357725420158976.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -21280442583053828053285208064.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -23324755033488097466294206464.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -25276502017308397528646942720.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -26747382228094288793002049536.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -29545832994010426198412230656.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -30947348724712686679045963776.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -33798909585278417549351976960.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -34977236831267292857896009728.000000\n', 'Train Epoch: 1 [53760/60000 (89%)]\tLoss: -37288570772108943489868234752.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -38863024285780373247872729088.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -41103397586667183156041351168.000000\n', '\n', 'Test set: Average loss: -42441783411297508080736010240.0000, Accuracy: 1135/10000 (11%)\n', '\n', '{"accuracy": 11.35, "runtime": 59.94434227794409, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}\n']
[2021-12-11 15:20:29,379 DEBUG] Received error from gpu2: []
[2021-12-11 15:20:29,380 DEBUG] SSH connection with gpu2 has been closed
[2021-12-11 15:20:29,380 INFO] {'batch-size': 256, 'lr': 0.5, 'gamma': 0.9} => {"accuracy": 11.35, "runtime": 59.94434227794409, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}
[2021-12-11 15:20:29,381 DEBUG] gpu2 finished training {'batch-size': 256, 'lr': 0.5, 'gamma': 0.9}
[2021-12-11 15:20:29,381 DEBUG] gpu2 now training {'batch-size': 256, 'lr': 0.9, 'gamma': 0.9}...
[2021-12-11 15:20:29,381 DEBUG] Attempting to establish SSH connection with gpu2...
[2021-12-11 15:20:29,487 DEBUG] SSH connection with gpu2 established successfully!
[2021-12-11 15:20:29,488 DEBUG] Running cmd on gpu2: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "256" --lr "0.9" --gamma "0.9"
[2021-12-11 15:20:29,488 INFO] Training {'batch-size': 256, 'lr': 0.9, 'gamma': 0.9} on gpu2...
[2021-12-11 15:21:15,074 DEBUG] Received output from gpu3: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002335\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -207571594707468288.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -59861607592257179214675968.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -1662778590331687346611683328.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -4472583991307979634585370624.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -7242302698950717401104121856.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -9677831336516977549962641408.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -12536593805969084629219540992.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -15077974288248867875543056384.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -17926327481381109539337666560.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -20246676419838558324022837248.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -22989051444953860127342985216.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -26105478035627542223572172800.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -28742062606793598234815102976.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -31527241161709630502352191488.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -33865287168561633282475687936.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -35845519467006603044757438464.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -39403872196696950844552118272.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -41449159815809932839297744896.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -45535706875426009021421715456.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -46873784695980640123077263360.000000\n', 'Train Epoch: 1 [53760/60000 (89%)]\tLoss: -50355519390669669364102332416.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -52088089540103782017974403072.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -54792036307964732083368099840.000000\n', '\n', 'Test set: Average loss: -56894235292516642125913260032.0000, Accuracy: 982/10000 (10%)\n', '\n', '{"accuracy": 9.82, "runtime": 57.36915805004537, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}\n']
[2021-12-11 15:21:15,074 DEBUG] Received error from gpu3: []
[2021-12-11 15:21:15,074 DEBUG] SSH connection with gpu3 has been closed
[2021-12-11 15:21:15,074 INFO] {'batch-size': 256, 'lr': 0.9, 'gamma': 0.5} => {"accuracy": 9.82, "runtime": 57.36915805004537, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}
[2021-12-11 15:21:15,075 DEBUG] gpu3 finished training {'batch-size': 256, 'lr': 0.9, 'gamma': 0.5}
[2021-12-11 15:21:15,075 DEBUG] gpu3 now training {'batch-size': 256, 'lr': 0.99, 'gamma': 0.5}...
[2021-12-11 15:21:15,076 DEBUG] Attempting to establish SSH connection with gpu3...
[2021-12-11 15:21:15,179 DEBUG] SSH connection with gpu3 established successfully!
[2021-12-11 15:21:15,179 DEBUG] Running cmd on gpu3: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "256" --lr "0.99" --gamma "0.5"
[2021-12-11 15:21:15,179 INFO] Training {'batch-size': 256, 'lr': 0.99, 'gamma': 0.5} on gpu3...
[2021-12-11 15:21:18,522 DEBUG] Received output from gpu1: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002335\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -207571594707468288.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -59861607592257179214675968.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -1662778590331687346611683328.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -4472583991307979634585370624.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -7242302698950717401104121856.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -9677831336516977549962641408.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -12536593805969084629219540992.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -15077974288248867875543056384.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -17926327481381109539337666560.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -20246676419838558324022837248.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -22989051444953860127342985216.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -26105478035627542223572172800.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -28742062606793598234815102976.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -31527241161709630502352191488.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -33865287168561633282475687936.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -35845519467006603044757438464.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -39403872196696950844552118272.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -41449159815809932839297744896.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -45535706875426009021421715456.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -46873784695980640123077263360.000000\n', 'Train Epoch: 1 [53760/60000 (89%)]\tLoss: -50355519390669669364102332416.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -52088089540103782017974403072.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -54792036307964732083368099840.000000\n', '\n', 'Test set: Average loss: -56894235292516642125913260032.0000, Accuracy: 982/10000 (10%)\n', '\n', '{"accuracy": 9.82, "runtime": 56.09388975659385, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}\n']
[2021-12-11 15:21:18,522 DEBUG] Received error from gpu1: []
[2021-12-11 15:21:18,523 DEBUG] SSH connection with gpu1 has been closed
[2021-12-11 15:21:18,523 INFO] {'batch-size': 256, 'lr': 0.9, 'gamma': 0.7} => {"accuracy": 9.82, "runtime": 56.09388975659385, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}
[2021-12-11 15:21:18,524 DEBUG] gpu1 finished training {'batch-size': 256, 'lr': 0.9, 'gamma': 0.7}
[2021-12-11 15:21:18,524 DEBUG] gpu1 now training {'batch-size': 256, 'lr': 0.99, 'gamma': 0.7}...
[2021-12-11 15:21:18,524 DEBUG] Attempting to establish SSH connection with gpu1...
[2021-12-11 15:21:18,619 DEBUG] SSH connection with gpu1 established successfully!
[2021-12-11 15:21:18,619 DEBUG] Running cmd on gpu1: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "256" --lr "0.99" --gamma "0.7"
[2021-12-11 15:21:18,619 INFO] Training {'batch-size': 256, 'lr': 0.99, 'gamma': 0.7} on gpu1...
[2021-12-11 15:21:29,049 DEBUG] Received output from gpu2: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002335\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -207571594707468288.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -59861607592257179214675968.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -1662778590331687346611683328.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -4472583991307979634585370624.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -7242302698950717401104121856.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -9677831336516977549962641408.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -12536593805969084629219540992.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -15077974288248867875543056384.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -17926327481381109539337666560.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -20246676419838558324022837248.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -22989051444953860127342985216.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -26105478035627542223572172800.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -28742062606793598234815102976.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -31527241161709630502352191488.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -33865287168561633282475687936.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -35845519467006603044757438464.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -39403872196696950844552118272.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -41449159815809932839297744896.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -45535706875426009021421715456.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -46873784695980640123077263360.000000\n', 'Train Epoch: 1 [53760/60000 (89%)]\tLoss: -50355519390669669364102332416.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -52088089540103782017974403072.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -54792036307964732083368099840.000000\n', '\n', 'Test set: Average loss: -56894235292516642125913260032.0000, Accuracy: 982/10000 (10%)\n', '\n', '{"accuracy": 9.82, "runtime": 56.43715795408934, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}\n']
[2021-12-11 15:21:29,049 DEBUG] Received error from gpu2: []
[2021-12-11 15:21:29,049 DEBUG] SSH connection with gpu2 has been closed
[2021-12-11 15:21:29,049 INFO] {'batch-size': 256, 'lr': 0.9, 'gamma': 0.9} => {"accuracy": 9.82, "runtime": 56.43715795408934, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}
[2021-12-11 15:21:29,050 DEBUG] gpu2 finished training {'batch-size': 256, 'lr': 0.9, 'gamma': 0.9}
[2021-12-11 15:21:29,050 DEBUG] gpu2 now training {'batch-size': 256, 'lr': 0.99, 'gamma': 0.9}...
[2021-12-11 15:21:29,050 DEBUG] Attempting to establish SSH connection with gpu2...
[2021-12-11 15:21:29,152 DEBUG] SSH connection with gpu2 established successfully!
[2021-12-11 15:21:29,152 DEBUG] Running cmd on gpu2: source /u5/p3basta/CS848/CS848-Project/venv/bin/activate && python /u5/p3basta/CS848/CS848-Project/models/MNIST/train.py --arch "alexnet" --parallelism "mp" --epochs "1" --batch-size "256" --lr "0.99" --gamma "0.9"
[2021-12-11 15:21:29,152 INFO] Training {'batch-size': 256, 'lr': 0.99, 'gamma': 0.9} on gpu2...
[2021-12-11 15:22:14,667 DEBUG] Received output from gpu3: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002335\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -518507933944774656.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -87954167977167510652846080.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -1815199461820219091645890560.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -4068673787922130113048608768.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -6261520897377434732433768448.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -8291042305691546057927819264.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -10747589015881438280221196288.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -12624688372115397143269736448.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -14947953371876017933874364416.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -16899198604257513096423145472.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -19228566082105622186055172096.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -21269425302049293171001655296.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -23384396160983499650520580096.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -26166172250848624338681200640.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -28146697336015532018966200320.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -29874448310453876199897694208.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -32522843520193589193820078080.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -33923990906310185842127142912.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -37286044306040608229678907392.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -38529424511514254335864209408.000000\n', 'Train Epoch: 1 [53760/60000 (89%)]\tLoss: -41459440407643140056927961088.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -42607801877114966031768485888.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -45917925773816512365727842304.000000\n', '\n', 'Test set: Average loss: -47003875344705923091066257408.0000, Accuracy: 982/10000 (10%)\n', '\n', '{"accuracy": 9.82, "runtime": 57.448675252497196, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}\n']
[2021-12-11 15:22:14,668 DEBUG] Received error from gpu3: []
[2021-12-11 15:22:14,668 DEBUG] SSH connection with gpu3 has been closed
[2021-12-11 15:22:14,668 INFO] {'batch-size': 256, 'lr': 0.99, 'gamma': 0.5} => {"accuracy": 9.82, "runtime": 57.448675252497196, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}
[2021-12-11 15:22:14,668 DEBUG] gpu3 finished training {'batch-size': 256, 'lr': 0.99, 'gamma': 0.5}
[2021-12-11 15:22:18,506 DEBUG] Received output from gpu1: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002335\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -518507933944774656.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -87954167977167510652846080.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -1815199461820219091645890560.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -4068673787922130113048608768.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -6261520897377434732433768448.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -8291042305691546057927819264.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -10747589015881438280221196288.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -12624688372115397143269736448.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -14947953371876017933874364416.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -16899198604257513096423145472.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -19228566082105622186055172096.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -21269425302049293171001655296.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -23384396160983499650520580096.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -26166172250848624338681200640.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -28146697336015532018966200320.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -29874448310453876199897694208.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -32522843520193589193820078080.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -33923990906310185842127142912.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -37286044306040608229678907392.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -38529424511514254335864209408.000000\n', 'Train Epoch: 1 [53760/60000 (89%)]\tLoss: -41459440407643140056927961088.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -42607801877114966031768485888.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -45917925773816512365727842304.000000\n', '\n', 'Test set: Average loss: -47003875344705923091066257408.0000, Accuracy: 982/10000 (10%)\n', '\n', '{"accuracy": 9.82, "runtime": 56.797824626322836, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}\n']
[2021-12-11 15:22:18,507 DEBUG] Received error from gpu1: []
[2021-12-11 15:22:18,507 DEBUG] SSH connection with gpu1 has been closed
[2021-12-11 15:22:18,507 INFO] {'batch-size': 256, 'lr': 0.99, 'gamma': 0.7} => {"accuracy": 9.82, "runtime": 56.797824626322836, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}
[2021-12-11 15:22:18,508 DEBUG] gpu1 finished training {'batch-size': 256, 'lr': 0.99, 'gamma': 0.7}
[2021-12-11 15:22:29,499 DEBUG] Received output from gpu2: ['Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.002335\n', 'Train Epoch: 1 [2560/60000 (4%)]\tLoss: -518507933944774656.000000\n', 'Train Epoch: 1 [5120/60000 (9%)]\tLoss: -87954167977167510652846080.000000\n', 'Train Epoch: 1 [7680/60000 (13%)]\tLoss: -1815199461820219091645890560.000000\n', 'Train Epoch: 1 [10240/60000 (17%)]\tLoss: -4068673787922130113048608768.000000\n', 'Train Epoch: 1 [12800/60000 (21%)]\tLoss: -6261520897377434732433768448.000000\n', 'Train Epoch: 1 [15360/60000 (26%)]\tLoss: -8291042305691546057927819264.000000\n', 'Train Epoch: 1 [17920/60000 (30%)]\tLoss: -10747589015881438280221196288.000000\n', 'Train Epoch: 1 [20480/60000 (34%)]\tLoss: -12624688372115397143269736448.000000\n', 'Train Epoch: 1 [23040/60000 (38%)]\tLoss: -14947953371876017933874364416.000000\n', 'Train Epoch: 1 [25600/60000 (43%)]\tLoss: -16899198604257513096423145472.000000\n', 'Train Epoch: 1 [28160/60000 (47%)]\tLoss: -19228566082105622186055172096.000000\n', 'Train Epoch: 1 [30720/60000 (51%)]\tLoss: -21269425302049293171001655296.000000\n', 'Train Epoch: 1 [33280/60000 (55%)]\tLoss: -23384396160983499650520580096.000000\n', 'Train Epoch: 1 [35840/60000 (60%)]\tLoss: -26166172250848624338681200640.000000\n', 'Train Epoch: 1 [38400/60000 (64%)]\tLoss: -28146697336015532018966200320.000000\n', 'Train Epoch: 1 [40960/60000 (68%)]\tLoss: -29874448310453876199897694208.000000\n', 'Train Epoch: 1 [43520/60000 (72%)]\tLoss: -32522843520193589193820078080.000000\n', 'Train Epoch: 1 [46080/60000 (77%)]\tLoss: -33923990906310185842127142912.000000\n', 'Train Epoch: 1 [48640/60000 (81%)]\tLoss: -37286044306040608229678907392.000000\n', 'Train Epoch: 1 [51200/60000 (85%)]\tLoss: -38529424511514254335864209408.000000\n', 'Train Epoch: 1 [53760/60000 (89%)]\tLoss: -41459440407643140056927961088.000000\n', 'Train Epoch: 1 [56320/60000 (94%)]\tLoss: -42607801877114966031768485888.000000\n', 'Train Epoch: 1 [58880/60000 (98%)]\tLoss: -45917925773816512365727842304.000000\n', '\n', 'Test set: Average loss: -47003875344705923091066257408.0000, Accuracy: 982/10000 (10%)\n', '\n', '{"accuracy": 9.82, "runtime": 57.22781403781846, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}\n']
[2021-12-11 15:22:29,499 DEBUG] Received error from gpu2: []
[2021-12-11 15:22:29,500 DEBUG] SSH connection with gpu2 has been closed
[2021-12-11 15:22:29,500 INFO] {'batch-size': 256, 'lr': 0.99, 'gamma': 0.9} => {"accuracy": 9.82, "runtime": 57.22781403781846, "mem_params": 244341408, "mem_bufs": 0, "mem_peak": 2154764288}
[2021-12-11 15:22:29,500 DEBUG] gpu2 finished training {'batch-size': 256, 'lr': 0.99, 'gamma': 0.9}
[2021-12-11 15:22:29,566 DEBUG] All trials have now been executed!
All Trial Results:
{'batch-size': 64, 'lr': 0.5, 'gamma': 0.7}: 9.8, runtime: 60.58595617301762, mem_params: 244341408, mem_bufs: 0, mem_peak: 2156343296
{'batch-size': 64, 'lr': 0.5, 'gamma': 0.5}: 9.8, runtime: 60.009412026032805, mem_params: 244341408, mem_bufs: 0, mem_peak: 2156343296
{'batch-size': 64, 'lr': 0.5, 'gamma': 0.9}: 9.8, runtime: 60.529757336713374, mem_params: 244341408, mem_bufs: 0, mem_peak: 2156343296
{'batch-size': 64, 'lr': 0.9, 'gamma': 0.5}: 9.8, runtime: 61.567034523934126, mem_params: 244341408, mem_bufs: 0, mem_peak: 2156343296
{'batch-size': 64, 'lr': 0.9, 'gamma': 0.7}: 9.8, runtime: 60.26496073510498, mem_params: 244341408, mem_bufs: 0, mem_peak: 2156343296
{'batch-size': 64, 'lr': 0.9, 'gamma': 0.9}: 9.8, runtime: 60.04498504800722, mem_params: 244341408, mem_bufs: 0, mem_peak: 2156343296
{'batch-size': 64, 'lr': 0.99, 'gamma': 0.5}: 9.8, runtime: 60.48147195391357, mem_params: 244341408, mem_bufs: 0, mem_peak: 2156343296
{'batch-size': 64, 'lr': 0.99, 'gamma': 0.7}: 9.8, runtime: 59.756867287214845, mem_params: 244341408, mem_bufs: 0, mem_peak: 2156343296
{'batch-size': 64, 'lr': 0.99, 'gamma': 0.9}: 9.8, runtime: 62.047039307653904, mem_params: 244341408, mem_bufs: 0, mem_peak: 2156343296
{'batch-size': 128, 'lr': 0.5, 'gamma': 0.5}: 10.09, runtime: 57.63888473622501, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154813440
{'batch-size': 128, 'lr': 0.5, 'gamma': 0.7}: 10.09, runtime: 57.32076796237379, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154813440
{'batch-size': 128, 'lr': 0.5, 'gamma': 0.9}: 10.09, runtime: 58.421860055066645, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154813440
{'batch-size': 128, 'lr': 0.9, 'gamma': 0.5}: 9.8, runtime: 58.477564914152026, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154813440
{'batch-size': 128, 'lr': 0.9, 'gamma': 0.7}: 9.8, runtime: 57.65384897682816, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154813440
{'batch-size': 128, 'lr': 0.9, 'gamma': 0.9}: 9.8, runtime: 58.678995272144675, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154813440
{'batch-size': 128, 'lr': 0.99, 'gamma': 0.5}: 9.8, runtime: 57.83978778868914, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154813440
{'batch-size': 128, 'lr': 0.99, 'gamma': 0.7}: 9.8, runtime: 58.865964769851416, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154813440
{'batch-size': 128, 'lr': 0.99, 'gamma': 0.9}: 9.8, runtime: 57.36283093178645, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154813440
{'batch-size': 256, 'lr': 0.5, 'gamma': 0.5}: 11.35, runtime: 57.599145675078034, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154764288
{'batch-size': 256, 'lr': 0.5, 'gamma': 0.7}: 11.35, runtime: 56.62067863531411, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154764288
{'batch-size': 256, 'lr': 0.5, 'gamma': 0.9}: 11.35, runtime: 59.94434227794409, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154764288
{'batch-size': 256, 'lr': 0.9, 'gamma': 0.5}: 9.82, runtime: 57.36915805004537, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154764288
{'batch-size': 256, 'lr': 0.9, 'gamma': 0.7}: 9.82, runtime: 56.09388975659385, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154764288
{'batch-size': 256, 'lr': 0.9, 'gamma': 0.9}: 9.82, runtime: 56.43715795408934, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154764288
{'batch-size': 256, 'lr': 0.99, 'gamma': 0.5}: 9.82, runtime: 57.448675252497196, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154764288
{'batch-size': 256, 'lr': 0.99, 'gamma': 0.7}: 9.82, runtime: 56.797824626322836, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154764288
{'batch-size': 256, 'lr': 0.99, 'gamma': 0.9}: 9.82, runtime: 57.22781403781846, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154764288
Best Trial Result:
{'batch-size': 256, 'lr': 0.5, 'gamma': 0.5}: 11.35, runtime: 57.599145675078034, mem_params: 244341408, mem_bufs: 0, mem_peak: 2154764288
